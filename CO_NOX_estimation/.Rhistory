# cioè per valori di lambda vicini allo zero
dev.new()
plot(cvridge_poly.mod)
bestlambda_ridge_poly = cvridge_poly.mod$lambda.min #  0.01
lambda1SE_ridge_poly = cvridge_poly.mod$lambda.1se # 1.149757
# Coefficienti praticamente uguali a quelli del modello lineare
coef(cvridge_poly.mod, bestlambda_ridge_poly) # coefficienti del modello per il lambda migliore
ridge.mod = glmnet(x_train_poly, y_train_poly, alpha = 0, lambda = grid)
dev.new()
plot(ridge.mod, label = T, xvar = "lambda")
#### ---- Lasso + Polinomio ---- ####
# K = 10
set.seed(1)
cvlasso_poly.mod = cv.glmnet(x_train_poly, y_train_poly, alpha = 1, lambda = grid)
# Osserviamo come il lambda migliore si ottiene per valori piccoli, interessante è osservare
# il lambda+1SE, infatti per quel valore di lambda otteniamo prestazioni sul test set paragonabili
# ma con un modello con soli 3 regressori
dev.new()
plot(cvlasso_poly.mod)
bestlambda_lasso_poly = cvlasso_poly.mod$lambda.min # 0.01
lambda1SE_lasso_poly = cvlasso_poly.mod$lambda.1se # 0.2154435
# Osserviamo come i coefficienti diversi da zero sono proprio quelli legati al polinomio
coef(cvlasso_poly.mod, lambda1SE_lasso_poly)
lasso.mod = glmnet(x_train_poly, y_train_poly, alpha = 1, lambda = grid)
dev.new()
plot(lasso.mod, label = T, xvar = "lambda")
#### ---- Best Lasso+Poly vs Best Ridge+Poly ---- ####
# Osserviamo ancora come con l'aggiunta del polinomio ci permette di ottenere ancora
# un lieve miglioramento delle performance a giustificazione ancora del fatto come
# il regressore TIT è fortemente legato all'uscita e come questo è legato in maniera non lineare
# alla stessa.
# Le prestazioni tra Ridge e Lasso sono paragonabili, leggermente migliori per Ridge che però
# ha tutti i regressori rispetto a Lasso che ne ha solo 3.
lasso_poly.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x_test_poly)
ridge_poly.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x_test_poly)
lasso_poly.pred_mse = mean((lasso_poly.pred-y_test_poly)^2) #  2.131106 # 2.21
ridge_poly.pred_mse = mean((ridge_poly.pred-y_test_poly)^2) #  2.042447 # 1.98
lasso_poly.pred_mse
ridge_poly.pred_mse
lasso.mod
lambda1SE_lasso
lambda1SE_ridge
lasso_poly.pred_mse
ridge_poly.pred_mse
ds = read.csv("dataset.csv")
ds_train = read.csv("ds_train.csv")
ds_test = read.csv("ds_test.csv")
ds_example = subset(ds, select = c("TIT", "GTEP"))
pr.out=prcomp(ds_example, scale=TRUE)
names(pr.out)
#### ---- PCA ---- ####
# Osservando la matrice di correlazione e gli scatter plot si osserva che alcuni regressori presentano
# una relazione lineare, questo significa che andando ad eseguire il calcolo delle componenti principali
# rispetto a questi regressori si potrà notare come un numero minimo di componeneti possa spiegare la loro
# variabilità.
# Un esempio possono essere i predittori TIT e GTEP (fortemente correlati) e come contro esempio TIT e AP (meno correlati)
ds_example = subset(ds, select = c("TIT", "GTEP"))
ds = read.csv("dataset.csv")
ds_train = read.csv("ds_train.csv")
ds_test = read.csv("ds_test.csv")
ds_example = subset(ds, select = c("TIT", "GTEP"))
pr.out=prcomp(ds_example, scale=TRUE)
names(pr.out)
dev.new()
par(mfrow = c(1,2))
plot(pr.out$x[, 1],ds_example[, "TIT"], xlab = "1st Principal Component Scores", ylab = "TIT", col = "green")
plot(pr.out$x[, 1],ds_example[, "GTEP"], xlab = "1st Principal Component Scores", ylab = "GTEP", col = "green")
dev.new()
par(mfrow = c(1,2))
plot(pr.out$x[, 2],ds_example[, "TIT"], xlab = "2nd Principal Component Scores", ylab = "TIT", col = "green")
plot(pr.out$x[, 2],ds_example[, "GTEP"], xlab = "2nd Principal Component Scores", ylab = "GTEP", col = "green")
pr.out$sdev
pr.var=pr.out$sdev^2; pr.var
pve=pr.var/sum(pr.var)
pve
ds_example = subset(ds, select = c("TIT", "AP"))
pr.out=prcomp(ds_example, scale=TRUE)
dev.new()
par(mfrow = c(1,2))
plot(pr.out$x[, 1],ds_example[, "TIT"], xlab = "1st Principal Component Scores", ylab = "TIT", col = "blue")
plot(pr.out$x[, 1],ds_example[, "AP"], xlab = "1st Principal Component Scores", ylab = "AP", col = "blue")
dev.new()
par(mfrow = c(1,2))
plot(pr.out$x[, 2],ds_example[, "TIT"], xlab = "2nd Principal Component Scores", ylab = "TIT", col = "blue")
plot(pr.out$x[, 2],ds_example[, "AP"], xlab = "2nd Principal Component Scores", ylab = "AP", col = "blue")
pr.out$sdev
pr.var=pr.out$sdev^2; pr.var
pve=pr.var/sum(pr.var)
pve # 0.502695 0.497305
#### ---- PCR ---- ####
# Principal Compent Regression
library(pls)
pcr.fit = pcr(CO~., data = ds_train,scale = TRUE, validation = "CV")
# Osserviamo come già con 3 componenti abbiamo una spiegazione della varianza del dataset
# accettabile, la spiegazione di ciò va ricercato nel fatto ci sono regressori tra loro correlati
# portando la diminuzione della dimensionalità sulla base dei ragionamenti di sopra.
summary(pcr.fit)
# Si osserva come il numero minomo di componenti che ci assicura un valore di MSE valido
# è pari a 5, aggiungerne altri comporta una variazione del MSE non significativa
dev.new()
validationplot(pcr.fit, val.type = "MSEP", legendpos = "topright")
MSEP(pcr.fit)$val # 2.31
# Osserviamo come per quella zona a maggiore densità il fit è ragionevolmente buono,
# si osserva ancora l'influenza dei outliers e della presenza di non-linearità nella zona
# a maggiore densità.
dev.new()
plot(pcr.fit, ncomp = 5, asp = 1, line = TRUE)
min_pcr = 5
dev.new()
plot(pcr.fit, plottype = "scores", comps = 1:3)
explvar(pcr.fit)
pls.fit = plsr(CO~., data = ds_train,scale = TRUE, validation = "CV")
summary(pls.fit)
ev.new()
validationplot(pls.fit, val.type = "MSEP", legendpos = "topright")
MSEP(pls.fit)$val
dev.new()
validationplot(pls.fit, val.type = "MSEP", legendpos = "topright")
MSEP(pls.fit)$val
pls.fit = plsr(CO~., data = ds_train,scale = TRUE, validation = "CV")
summary(pls.fit)
# Si ottiene un MSEP ragionevole per un numero di componenti pari a 3.
# Il numero di compomenti determinanti è minore del modello ottenuto sopra
# in quanto la PLS nel calcolo delle componenti prende in considerazione
# anche la risposta CO.
dev.new()
validationplot(pls.fit, val.type = "MSEP", legendpos = "topright")
MSEP(pls.fit)$val # 2.24
dev.new()
plot(pls.fit, ncomp = 3, asp = 1, line = TRUE)
# a maggiore densità.
dev.new()
plot(pcr.fit, ncomp = 5, asp = 1, line = TRUE)
min_pcr = 5
# A parità di componenti la varianza spiegata dell'uscita è maggiore con l'introduzione del
# polinomio.
set.seed(1)
pls_poly.fit = plsr(CO~.-TIT+poly(TIT,3), data = ds_train,scale = TRUE, validation = "CV")
summary(pls_poly.fit)
# Si osserva come con l'introduzione del polinomio il valore del MSEP diminuisce in maniera significativa
# in particolare valori ragionevoli si ottengono per un numero di componenti pari a 3
dev.new()
validationplot(pls_poly.fit, val.type = "MSEP", legendpos = "topright")
MSEP(pls_poly.fit)$val # 1.94 #PC = 3
dev.new()
plot(pls_poly.fit, ncomp = 3, asp = 1, line = TRUE)
pcr_prediction = predict(pcr.fit, ds[test_set, ], ncomp = 5)
pcr_mse_test =  mean((pcr_prediction-ds[test_set, "CO"])^2) # 2.442668
pls_prediction = predict(pls.fit, ds[test_set, ], ncomp = 3)
pls_mse_test =  mean((pls_prediction-ds[test_set, "CO"])^2) #  2.433534
pls_poly.fit_prediction = predict(pls_poly.fit, ds[test_set, ], ncomp = 3)
pls_mse_test =  mean((pls_poly.fit_prediction-ds[test_set, "CO"])^2) #  2.10267
dev.new()
plot(pls_poly.fit, ncomp = 3, asp = 1, line = TRUE)
#### ---- PCR vs PLS vs PLS+Poly ---- ####
# Osserviamo come le prestazioni migliori tra i modelli migliori ottenuti con la tecnica PCA
# si ottengono ancora una volta con la presenza di una trasformata non lineare.
# Osserviamo come l'andamente delle prestazioni è ancora coerente con quelli ottenuti con gli
# altri modelli, di conseguenza per motivi di facilità di interpretazione si andrà comunque
# a scegliere un modello ottenuto con le tecniche classiche di analisi dei regressori.
pcr_prediction = predict(pcr.fit, ds_test, ncomp = 5)
pcr_mse_test =  mean((pcr_prediction-ds_test[ , "CO"])^2) # 2.442668
pcr_mse_test
pls_prediction = predict(pls.fit, ds_test, ncomp = 3)
pls_mse_test =  mean((pls_prediction-ds_test[ , "CO"])^2) #  2.433534
pls_mse_test
pls_poly.fit_prediction = predict(pls_poly.fit, ds_test, ncomp = 3)
pls_mse_test =  mean((pls_poly.fit_prediction-ds_test[ , "CO"])^2) #  2.10267
pls_mse_test
#### ---- Shrinkage Method ---- ####
# Gli schrinkage methods (Ridge e Lasso) vengono applicati in un contesto in cui
# p << n e dove il dataset presenta problemi di multicollinearità.
# Questo significa che in generale i metodi di shrinkage dovrebbero darci prestazioni migliori
# rispetto al fit Least Square che comprende tutti i regressori.
#### ---- Library ---- ####
ds = read.csv("dataset.csv")
ds_train = read.csv("ds_train.csv")
ds_test = read.csv("ds_test.csv")
library(glmnet)
x_train  = model.matrix(CO~., ds_train)[, -1]
y_train = ds_train$CO
x_test  = model.matrix(CO~., ds_test)[, -1]
y_test = ds_test$CO
# Vettore dei coefficienti lambda
grid = 10^seq(10,-2, length = 100)
# Sia la tecnica ridge che la tecnica lasso hanno un coefficiente lambda da determinare
# la tecnica utilizzata per la stima del lambda migliore è la Cross Validation.
# Sul training andiamo ad eseguire la CV per individuare il lambda miglire
# Sul test andremmo a valutare il modello ottenuto per fare un confronto Ridge vs Lasso
#### ---- Ridge Regression ---- ####
# K = 10
set.seed(1)
cvridge.mod = cv.glmnet(x_train, y_train, alpha = 0, lambda = grid)
# Osserviamo come all'aumentare del log(lamda) quindi all'aumentare di lamda aumenta il MSE di test predetto
# questo significa che andando a ridurre l'importanza di alcuni regressori si ottiene una capacità di predizione
# peggiore
dev.new()
plot(cvridge.mod)
# Osserviamo come il lambda a 1se è pari a 0.6579332,
# ancora capiamo che l'influenza dell'indice di regolarizzazione è praticamente inutile
bestlambda_ridge = cvridge.mod$lambda.min
lambda1SE_ridge = cvridge.mod$lambda.1se
# Il miglior lambda si ottiene su 0.01, praticamente pari a 0, cioè pari alla regressione least square classica
# Ciò si potrebbe giustificare con il fatto che i coefficienti del modello least square con tutti i regressori
# sono già relativamente bassi.
lm_all = lm(CO~., data = ds_train)
coef(lm_all) # coefficienti modello Least Square
coef(cvridge.mod, bestlambda_ridge) # coefficienti del modello per il lambda migliore
# Osserviamo come i coefficienti partono da valori piccoli, quindi all'aumentare del lambda si vanno
# a stringere.
ridge.mod = glmnet(x_train, y_train, alpha = 0, lambda = grid)
dev.new()
plot(ridge.mod, label = T, xvar = "lambda")
#### ---- Lasso ---- ####
# Si osserva come il fattore di regolarizzazione lambda rimane piccolo
# però adesso si può osservare la capacità di effettuare variable selection
# della tecnica Lasso, infatti osserviamo come in corrispondenza di lambda+1SE
# i coefficienti irrilevanti si portano a 0, mantenendo diversi da 0 solo
# TIT e TAT, andando a confermare le intuizioni fatte in least_square.R
set.seed(1)
cvlasso.mod = cv.glmnet(x_train, y_train, alpha = 1, lambda = grid)
dev.new()
plot(cvlasso.mod)
bestlambda_lasso = cvlasso.mod$lambda.min # 0.01
lambda1SE_lasso = cvlasso.mod$lambda.1se # 0.2154435
coef(cvlasso.mod, bestlambda_lasso) # coefficienti del modello per il lambda migliore
coef(cvlasso.mod, lambda1SE_lasso) # coefficienti del modello per il lambda + 1SE
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid)
dev.new()
plot(lasso.mod, label = T, xvar = "lambda")
#### ---- Best Lasso vs Best Ridge ---- ####
# Andiamo a confrontare il Best Ridge (praticamente uguale al Least Square) e il Best Lasso
# al fine di valutare le prestazioni di predizione sul Test Set
# Osserviamo come le performance tra Ridge e Lasso sono tra loro paragonabili
# quindi con Lasso con solo due coefficienti raggiungiamo prestazioni simili a quello
# ridge
lasso.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x_test)
ridge.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x_test)
lasso.pred_mse = mean((lasso.pred-y_test)^2) #  2.47327
ridge.pred_mse = mean((ridge.pred-y_test)^2) #  2.449322
lasso.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x_train)
ridge.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x_train)
lasso.pred_mes_tr = mean((lasso.pred-y_train)^2) # 2.452729
ridge.pred_mes_tr = mean((ridge.pred-y_train)^2) # 2.431468
#### ---- Introduzione del polinomio ---- ####
# Fissata la presenza di una relazione non lineare tra i regressori e l'uscita
# si andrà a provare il ridge e lasso shrinkage method sul modello che introduce anche
# il polinomio sulla variabile TIT, aspettandoci di osservare un comportamento simile
# a quello di sopra, quindi con un fattore di regolarizzazione basso per Ridge, in quanto
# dalla analisi sul least square già si poteva osservare come il polinomio dominava gli altri
# coefficienti che erano molto vicino a zero, nel caso del Lasso, ci aspettiamo la selezione del solo polinomio
# dato comunque un fattore di smorzamento basso
x_train_poly  = model.matrix(CO~.-TIT+poly(TIT,3), ds_train)[, -1]
y_train_poly = ds_train$CO
x_test_poly  = model.matrix(CO~.-TIT+poly(TIT,3), ds_test)[, -1]
y_test_poly = ds_test$CO
#### ---- Ridge + Polinomio ---- ####
# K = 10
set.seed(1)
cvridge_poly.mod = cv.glmnet(x_train_poly, y_train_poly, alpha = 0, lambda = grid)
# come era lecito aspettarsi il lambda migliore si trova per valori del log(lambda) negativi
# cioè per valori di lambda vicini allo zero
dev.new()
plot(cvridge_poly.mod)
bestlambda_ridge_poly = cvridge_poly.mod$lambda.min #  0.01
lambda1SE_ridge_poly = cvridge_poly.mod$lambda.1se # 1.149757
# Coefficienti praticamente uguali a quelli del modello lineare
coef(cvridge_poly.mod, bestlambda_ridge_poly) # coefficienti del modello per il lambda migliore
ridge.mod = glmnet(x_train_poly, y_train_poly, alpha = 0, lambda = grid)
dev.new()
plot(ridge.mod, label = T, xvar = "lambda")
#### ---- Lasso + Polinomio ---- ####
# K = 10
set.seed(1)
cvlasso_poly.mod = cv.glmnet(x_train_poly, y_train_poly, alpha = 1, lambda = grid)
# Osserviamo come il lambda migliore si ottiene per valori piccoli, interessante è osservare
# il lambda+1SE, infatti per quel valore di lambda otteniamo prestazioni sul test set paragonabili
# ma con un modello con soli 3 regressori
dev.new()
plot(cvlasso_poly.mod)
bestlambda_lasso_poly = cvlasso_poly.mod$lambda.min # 0.01
lambda1SE_lasso_poly = cvlasso_poly.mod$lambda.1se # 0.2154435
# Osserviamo come i coefficienti diversi da zero sono proprio quelli legati al polinomio
coef(cvlasso_poly.mod, lambda1SE_lasso_poly)
lasso.mod = glmnet(x_train_poly, y_train_poly, alpha = 1, lambda = grid)
dev.new()
plot(lasso.mod, label = T, xvar = "lambda")
#### ---- Best Lasso+Poly vs Best Ridge+Poly ---- ####
# Osserviamo ancora come con l'aggiunta del polinomio ci permette di ottenere ancora
# un lieve miglioramento delle performance a giustificazione ancora del fatto come
# il regressore TIT è fortemente legato all'uscita e come questo è legato in maniera non lineare
# alla stessa.
# Le prestazioni tra Ridge e Lasso sono paragonabili, leggermente migliori per Ridge che però
# ha tutti i regressori rispetto a Lasso che ne ha solo 3.
lasso_poly.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x_test_poly)
ridge_poly.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x_test_poly)
lasso_poly.pred_mse = mean((lasso_poly.pred-y_test_poly)^2) # 2.21
ridge_poly.pred_mse = mean((ridge_poly.pred-y_test_poly)^2) # 1.98
lasso_poly.pred_mse
ridge_poly.pred_mse
lambda1SE_ridge
#### ---- Best Lasso+Poly vs Best Ridge+Poly ---- ####
# Osserviamo ancora come con l'aggiunta del polinomio ci permette di ottenere ancora
# un lieve miglioramento delle performance a giustificazione ancora del fatto come
# il regressore TIT è fortemente legato all'uscita e come questo è legato in maniera non lineare
# alla stessa.
# Le prestazioni tra Ridge e Lasso sono paragonabili, leggermente migliori per Ridge che però
# ha tutti i regressori rispetto a Lasso che ne ha solo 3.
lasso_poly.pred = predict(lasso.mod, s = lambda1SE_lasso_poly, newx = x_test_poly)
ridge_poly.pred = predict(ridge.mod, s = lambda1SE_ridge_poly, newx = x_test_poly)
lasso_poly.pred_mse = mean((lasso_poly.pred-y_test_poly)^2) # 2.21
ridge_poly.pred_mse = mean((ridge_poly.pred-y_test_poly)^2) # 1.98
lasso_poly.pred_mse
ridge_poly.pred_mse
lambda1SE_lasso_poly
lambda1SE_ridge_poly
ridge_poly.pred_mse
x_train_poly
bestlambda_ridge_poly = cvridge_poly.mod$lambda.min #  0.01
lambda1SE_ridge_poly = cvridge_poly.mod$lambda.1se # 1.149757
bestlambda_ridge_poly
lambda1SE_ridge_poly
coef(cvridge_poly.mod, bestlambda_ridge_poly) # coefficienti del modello per il lambda migliore
names(ridge.mod)
plot(ridge.mod, label = T, xvar = "lambda")
ridge.mod = glmnet(x_train_poly, y_train_poly, alpha = 0, lambda = grid)
dev.new()
plot(ridge.mod, label = T, xvar = "lambda")
plot(ridge.mod, label = T, xvar = "lambda")
ds = read.csv("dataset.csv")
ds_train = read.csv("ds_train.csv")
ds_test = read.csv("ds_test.csv")
library(glmnet)
x_train  = model.matrix(CO~., ds_train)[, -1]
y_train = ds_train$CO
x_test  = model.matrix(CO~., ds_test)[, -1]
y_test = ds_test$CO
# Vettore dei coefficienti lambda
grid = 10^seq(10,-2, length = 100)
#### ---- Ridge Regression ---- ####
# K = 10
set.seed(1)
cvridge.mod = cv.glmnet(x_train, y_train, alpha = 0, lambda = grid)
# Osserviamo come all'aumentare del log(lamda) quindi all'aumentare di lamda aumenta il MSE di test predetto
# questo significa che andando a ridurre l'importanza di alcuni regressori si ottiene una capacità di predizione
# peggiore
dev.new()
plot(cvridge.mod)
# Osserviamo come il lambda a 1se è pari a 0.6579332,
# ancora capiamo che l'influenza dell'indice di regolarizzazione è praticamente inutile
bestlambda_ridge = cvridge.mod$lambda.min
lambda1SE_ridge = cvridge.mod$lambda.1se
bestlambda_ridge
lambda1SE_ridge
# Il miglior lambda si ottiene su 0.01, praticamente pari a 0, cioè pari alla regressione least square classica
# Ciò si potrebbe giustificare con il fatto che i coefficienti del modello least square con tutti i regressori
# sono già relativamente bassi.
lm_all = lm(CO~., data = ds_train)
coef(lm_all) # coefficienti modello Least Square
coef(cvridge.mod, bestlambda_ridge) # coefficienti del modello per il lambda migliore
# Osserviamo come i coefficienti partono da valori piccoli, quindi all'aumentare del lambda si vanno
# a stringere.
ridge.mod = glmnet(x_train, y_train, alpha = 0, lambda = grid)
dev.new()
plot(ridge.mod, label = T, xvar = "lambda")
#### ---- Lasso ---- ####
# Si osserva come il fattore di regolarizzazione lambda rimane piccolo
# però adesso si può osservare la capacità di effettuare variable selection
# della tecnica Lasso, infatti osserviamo come in corrispondenza di lambda+1SE
# i coefficienti irrilevanti si portano a 0, mantenendo diversi da 0 solo
# TIT e TAT, andando a confermare le intuizioni fatte in least_square.R
set.seed(1)
cvlasso.mod = cv.glmnet(x_train, y_train, alpha = 1, lambda = grid)
dev.new()
plot(cvlasso.mod)
bestlambda_lasso = cvlasso.mod$lambda.min # 0.01
bestlambda_lasso
lambda1SE_lasso
lambda1SE_lasso = cvlasso.mod$lambda.1se # 0.2154435
lambda1SE_lasso
coef(cvlasso.mod, lambda1SE_lasso)
y_test
lasso.pred_mse = mean((lasso.pred-y_test)^2) #  2.47327
lasso.pred_mse = mean((lasso.pred-y_test)^2) #  2.47327
#### ---- Best Lasso vs Best Ridge ---- ####
# Andiamo a confrontare il Best Ridge (praticamente uguale al Least Square) e il Best Lasso
# al fine di valutare le prestazioni di predizione sul Test Set
# Osserviamo come le performance tra Ridge e Lasso sono tra loro paragonabili
# quindi con Lasso con solo due coefficienti raggiungiamo prestazioni simili a quello
# ridge
lasso.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x_test)
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid)
#### ---- Best Lasso vs Best Ridge ---- ####
# Andiamo a confrontare il Best Ridge (praticamente uguale al Least Square) e il Best Lasso
# al fine di valutare le prestazioni di predizione sul Test Set
# Osserviamo come le performance tra Ridge e Lasso sono tra loro paragonabili
# quindi con Lasso con solo due coefficienti raggiungiamo prestazioni simili a quello
# ridge
lasso.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x_test)
ridge.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x_test)
lasso.pred_mse = mean((lasso.pred-y_test)^2) #  2.47327
ridge.pred_mse = mean((ridge.pred-y_test)^2) #  2.449322
lasso.pred_mse
ridge.pred_mse
lm_all = lm(CO~., data = ds_train)
lm_all_pred = predict(lm_all, newdata = ds_test)
lm_all_mse = mean((ds_test[,"CO"] - lm_all_pred)^2)
lm_all_mse
lm_all_mse = mean((y_test - lm_all_pred)^2)
lm_all_mse
coef(lm_all)
coef(ridge.mod, lambda1SE_ridge)
ridge.pred = predict(ridge.mod, s = bestlambda_ridge, newx = x_test)
lasso.pred_mse = mean((lasso.pred-y_test)^2) #  2.47327
ridge.pred_mse = mean((ridge.pred-y_test)^2) #  2.449322
ridge.pred_mse
coef(ridge.mod, lambda1SE_ridge)
coef(ridge.mod, lambda1SE_ridge)
lm_all_mse
ridge_poly.mod = glmnet(x_train_poly, y_train_poly, alpha = 0, lambda = grid)
x_train_poly  = model.matrix(CO~.-TIT+poly(TIT,3), ds_train)[, -1]
y_train_poly = ds_train$CO
x_test_poly  = model.matrix(CO~.-TIT+poly(TIT,3), ds_test)[, -1]
y_test_poly = ds_test$CO
#### ---- Ridge + Polinomio ---- ####
# K = 10
set.seed(1)
cvridge_poly.mod = cv.glmnet(x_train_poly, y_train_poly, alpha = 0, lambda = grid)
bestlambda_ridge_poly = cvridge_poly.mod$lambda.min #  0.01
lambda1SE_ridge_poly = cvridge_poly.mod$lambda.1se # 1.149757
bestlambda_ridge_poly
lambda1SE_ridge_poly
ridge_poly.mod = glmnet(x_train_poly, y_train_poly, alpha = 0, lambda = grid)
#### ---- Lasso + Polinomio ---- ####
# K = 10
set.seed(1)
cvlasso_poly.mod = cv.glmnet(x_train_poly, y_train_poly, alpha = 1, lambda = grid)
# Osserviamo come il lambda migliore si ottiene per valori piccoli, interessante è osservare
# il lambda+1SE, infatti per quel valore di lambda otteniamo prestazioni sul test set paragonabili
# ma con un modello con soli 3 regressori
dev.new()
bestlambda_lasso_poly = cvlasso_poly.mod$lambda.min # 0.01
lambda1SE_lasso_poly = cvlasso_poly.mod$lambda.1se # 0.2154435
lasso_poly.mod = glmnet(x_train_poly, y_train_poly, alpha = 1, lambda = grid)
#### ---- Best Lasso+Poly vs Best Ridge+Poly ---- ####
# Osserviamo ancora come con l'aggiunta del polinomio ci permette di ottenere ancora
# un lieve miglioramento delle performance a giustificazione ancora del fatto come
# il regressore TIT è fortemente legato all'uscita e come questo è legato in maniera non lineare
# alla stessa.
# Osserviamo come il ridge con la presenza del polinomio performa effettivamente meglio del lasso
# anche se il modello ridge presenta tutti i regressori.
lasso_poly.pred = predict(lasso_poly.mod, s = lambda1SE_lasso_poly, newx = x_test_poly)
ridge_poly.pred = predict(ridge_poly.mod, s = bestlambda_ridge_poly, newx = x_test_poly)
lasso_poly.pred_mse = mean((lasso_poly.pred-y_test_poly)^2) # 2.21
ridge_poly.pred_mse = mean((ridge_poly.pred-y_test_poly)^2) # 1.96
lasso_poly.pred_mse
ridge_poly.pred_mse
ridge_poly.pred = predict(ridge_poly.mod, s = lambda1SE_ridge_poly, newx = x_test_poly)
ridge_poly.pred_mse = mean((ridge_poly.pred-y_test_poly)^2) # 2.33
ridge_poly.pred_mse
coef(ridge_poly.mod, lambda1SE_ridge_poly)
coef(ridge_poly.mod, bestlambda_ridge_poly)
lambda1SE_ridge_poly
bestlambda_ridge_poly
bestlambda_ridge_poly
ridge_poly.pred = predict(ridge_poly.mod, s = bestlambda_ridge_poly, newx = x_test_poly)
ridge_poly.pred_mse = mean((ridge_poly.pred-y_test_poly)^2) # 2.33
ridge_poly.pred_mse
ridge_poly.pred = predict(ridge_poly.mod, s = bestlambda_ridge_poly, newx = x_test_poly)
ridge_poly.pred_mse = mean((ridge_poly.pred-y_test_poly)^2) # 2.33
ridge_poly.pred_mse
ridge_poly.pred = predict(ridge_poly.mod, s = lambda1SE_ridge_poly, newx = x_test_poly)
lasso_poly.pred_mse = mean((lasso_poly.pred-y_test_poly)^2) # 2.21
ridge_poly.pred_mse = mean((ridge_poly.pred-y_test_poly)^2) # 2.33
ridge_poly.pred_mse
ds = read.csv("dataset.csv")
ds_train = read.csv("ds_train.csv")
ds_test = read.csv("ds_test.csv")
glm_fit_all = glm(formula = CO~., data = ds_train)
fit_all_cv = cv.glm(ds_train, glm_fit_all, K = 10)$delta[1] #  2.244833
mean((predict(fit_all_cv, ds_test)-ds_test[,"CO"])^2)
library(boot)
#### ---- K-fold CV sui modelli generati ---- ####
# fit_all, fit1, fit_poly, fit1.poly2, fit1.poly3
# Si osserva che effettivamente l'introduzione della relazione non lineare
# effettivamente comporta dei miglioramenti.
glm_fit_all = glm(formula = CO~., data = ds_train)
fit_all_cv = cv.glm(ds_train, glm_fit_all, K = 10)$delta[1] #  2.244833
mean((predict(glm_fit_all, ds_test)-ds_test[,"CO"])^2)
#### ---- TIT + TAT + Poly(TIT) ---- ####
fit_tit_poly_tat = glm(formula = CO~TAT+poly(TIT), data = ds_train)
summary(fit_tit_poly_tat)
#### ---- TIT + TAT + Poly(TIT) ---- ####
fit_tit_poly_tat = lm(formula = CO~TAT+poly(TIT), data = ds_train)
summary(fit_tit_poly_tat)
#### ---- TIT + TAT + Poly(TIT) ---- ####
fit_tit_poly_tat = lm(formula = CO~TAT+poly(TIT,3), data = ds_train)
summary(fit_tit_poly_tat)
mse((ds_test[, "CO"]-predict(fit_tit_poly_tat))^2)
mean((ds_test[, "CO"]-predict(fit_tit_poly_tat))^2)
mean((ds_test[, "CO"]-predict(fit_tit_poly_tat, newdata = ds_test))^2)
ds1 = read.csv("Gas_turbine/gt_2011.csv")
ds2 = read.csv("Gas_turbine/gt_2012.csv")
ds3 = read.csv("Gas_turbine/gt_2013.csv")
ds4 = read.csv("Gas_turbine/gt_2014.csv")
ds5 = read.csv("Gas_turbine/gt_2015.csv")
ds = rbind(ds1, ds2)
ds = rbind(ds, ds3)
ds = rbind(ds, ds4)
ds = rbind(ds, ds5)
#### ---- Summary ---- ####
summary(ds)
