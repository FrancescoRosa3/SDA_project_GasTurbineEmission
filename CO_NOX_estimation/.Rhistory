#### ---- 2. Linear Fit ---- ####
fit = lm(CO~., data = ds_1_year)
summary(fit)
# Problema di collinearità
vif(fit)
attach(ds)
attach(ds_1_year)
#### ---- 2. Linear Fit ---- ####
fit = lm(CO~.-NOX, data = ds_1_year)
summary(fit)
dev.new()
par(mfrow = c(2,2))
plot(fit)
ds1 = read.csv("Gas_turbine/gt_2011.csv")
ds2 = read.csv("Gas_turbine/gt_2012.csv")
ds3 = read.csv("Gas_turbine/gt_2013.csv")
ds4 = read.csv("Gas_turbine/gt_2014.csv")
ds5 = read.csv("Gas_turbine/gt_2015.csv")
ds = rbind(ds1, ds2)
ds = rbind(ds, ds3)
ds = rbind(ds, ds4)
ds = rbind(ds, ds5)
library(corrplot)
library(RColorBrewer)
cor <-cor(ds)
dev.new()
corrplot(cor, type="upper", order="hclust",col=brewer.pal(n=8, name="RdYlBu"))
dev.new()
pairs(ds)
summary(ds_1_year)
dev.new()
pairs(ds)
attach(ds)
fit = lm(CO~.-NOX, data = ds)
summary(fit)
#### --- Features list ---- ####
names(ds)
#### ---- Summary ---- ####
summary(ds)
#### ---- Histograms --- ####
dev.new()
hist(ds)
View(ds)
#### ---- Boxplots ---- ####
dev.new()
boxplot(ds)
dev.new()
par(mfrow = c(2,2))
plot(fit)
hist(ds$AT);hist(ds$AP);hist(ds$AP);hist(ds$AP);
dev.new()
par(mfrow = c(2, 3))
plot(fit)
hist(ds$AT);hist(ds$AP);hist(ds$AH);hist(ds$AFDP);hist(ds$GTEP);hist(ds$TIT);
dev.new()
par(mfrow = c(2, 3))
plot(fit)
hist(ds$AT);hist(ds$AP);hist(ds$AH);hist(ds$AFDP);hist(ds$GTEP);hist(ds$TIT);
par(mfrow = c(2, 3))
hist(ds$AT);hist(ds$AP);hist(ds$AH);hist(ds$AFDP);hist(ds$GTEP);hist(ds$TIT);
dev.new()
par(mfrow = c(2, 3))
hist(ds$AT);hist(ds$AP);hist(ds$AH);hist(ds$AFDP);hist(ds$GTEP);hist(ds$TIT);
dev.new()
par(mfrow = c(2,3))
hist(ds$TAT);hist(ds$TEY);hist(ds$CDP);hist(ds$CO);hist(ds$NOX)
dev.new()
par(mfrow = c(2, 3))
hist(ds$AT);hist(ds$AP);hist(ds$AH);hist(ds$AFDP);hist(ds$GTEP);hist(ds$TIT);
dev.new()
par(mfrow = c(2, 3))
hist(ds$AT);hist(ds$AP);hist(ds$AH);hist(ds$AFDP);hist(ds$GTEP);hist(ds$TIT);
dev.new()
par(mfrow = c(2,3))
hist(ds$TAT);hist(ds$TEY);hist(ds$CDP);hist(ds$CO);hist(ds$NOX)
dev.new()
par(mfrow = c(2, 3))
boxplot(ds$AT);boxplot(ds$AP);boxplot(ds$AH);boxplot(ds$AFDP);boxplot(ds$GTEP);boxplot(ds$TIT);
dev.new()
par(mfrow = c(2,3))
boxplot(ds$TAT);boxplot(ds$TEY);boxplot(ds$CDP);boxplot(ds$CO);boxplot(ds$NOX)
boxplot(ds$AT, xlab = "AT")
boxplot(ds$AT, xlab = "AT", horizontal = TRUE)
boxplot(ds$AT, ylab = "AT", horizontal = TRUE)
dev.new()
par(mfrow = c(2, 3))
boxplot(ds$AT, ylab = "AT", horizontal = TRUE);boxplot(ds$AP, ylab = "AP", horizontal = TRUE);boxplot(ds$AH, ylab = "AH", horizontal = TRUE);boxplot(ds$AFDP, ylab = "AFDP", horizontal = TRUE);
boxplot(ds$GTEP, ylab = "GTEP", horizontal = TRUE);boxplot(ds$TIT, ylab = "TIT", horizontal = TRUE);
dev.new()
par(mfrow = c(2,3))
boxplot(ds$TAT, ylab = "TAT", horizontal = TRUE);boxplot(ds$TEY, ylab = "TEY", horizontal = TRUE);boxplot(ds$CDP, ylab = "CDP", horizontal = TRUE);
boxplot(ds$CO, ylab = "CO", horizontal = TRUE);boxplot(ds$NOX, ylab = "NOX", horizontal = TRUE)
#### ---- Scatter Plots ---- ####
dev.new()
pairs(ds)
dev.new()
par(mfrow = c(2, 3))
boxplot(ds$AT, ylab = "AT", horizontal = TRUE);boxplot(ds$AP, ylab = "AP", horizontal = TRUE);boxplot(ds$AH, ylab = "AH", horizontal = TRUE);boxplot(ds$AFDP, ylab = "AFDP", horizontal = TRUE);
boxplot(ds$GTEP, ylab = "GTEP", horizontal = TRUE);boxplot(ds$TIT, ylab = "TIT", horizontal = TRUE);
dev.new()
par(mfrow = c(2,3))
boxplot(ds$TAT, ylab = "TAT", horizontal = TRUE);boxplot(ds$TEY, ylab = "TEY", horizontal = TRUE);boxplot(ds$CDP, ylab = "CDP", horizontal = TRUE);
boxplot(ds$CO, ylab = "CO", horizontal = TRUE);boxplot(ds$NOX, ylab = "NOX", horizontal = TRUE)
dev.new()
par(mfrow = c(2, 3))
boxplot(ds$AT, ylab = "AT", horizontal = TRUE);boxplot(ds$AP, ylab = "AP", horizontal = TRUE);boxplot(ds$AH, ylab = "AH", horizontal = TRUE);boxplot(ds$AFDP, ylab = "AFDP", horizontal = TRUE);
boxplot(ds$GTEP, ylab = "GTEP", horizontal = TRUE);boxplot(ds$TIT, ylab = "TIT", horizontal = TRUE);
dev.new()
par(mfrow = c(2,3))
boxplot(ds$TAT, ylab = "TAT", horizontal = TRUE);boxplot(ds$TEY, ylab = "TEY", horizontal = TRUE);boxplot(ds$CDP, ylab = "CDP", horizontal = TRUE);
boxplot(ds$CO, ylab = "CO", horizontal = TRUE);boxplot(ds$NOX, ylab = "NOX", horizontal = TRUE)
w <- abs(rstudent(fit1)) < 3 & abs(cooks.distance(fit1)) < 4/nrow(fit1$model)
# Eliminiamo quei regressori con un VIF elevato
fit1 = lm(CO~.-NOX-TEY-CDP-GTEP, data = ds[train_set, ]);
x = c(1:nrow(ds))
train = (nrow(ds)*70)/100
validation = (nrow(ds)*30)/100
train_set = sample(x, train, rep = FALSE)
validation_set = sample(x[-train_set], validation, rep = FALSE)
#### ---- library ---- ####
library(car)
# Eliminiamo quei regressori con un VIF elevato
fit1 = lm(CO~.-NOX-TEY-CDP-GTEP, data = ds[train_set, ]);
vif(fit1)
w <- abs(rstudent(fit1)) < 3 & abs(cooks.distance(fit1)) < 4/nrow(fit1$model)
ds_train = ds[train_set,]
dim(ds_train)
ds_train_no_out = ds_train[w,]
fit2 <- lm(CO~.-NOX-TEY-CDP-GTEP, data = ds_train_no_out);
summary(fit2)
dev.new()
pairs(~ds_train_no_out$CO+ds_train_no_out$AT+ds_train_no_out$AP+ds_train_no_out$AH+ds_train_no_out$AFDP+ds_train_no_out$TIT+ds_train_no_out$TAT)
dev.new()
par(mfrow = c(2,2))
plot(fit1)
dev.new()
# Osservando ancora i grafici a coppie dei soli predittori rilevanti
# possiamo osservare come in realtà, la motivazione principale può essere data dalla presenza di relazioni non lineari
pairs(~CO+AT+AP+AH+AFDP+TIT+TAT)
attach(ds)
# Osservando ancora i grafici a coppie dei soli predittori rilevanti
# possiamo osservare come in realtà, la motivazione principale può essere data dalla presenza di relazioni non lineari
pairs(~CO+AT+AP+AH+AFDP+TIT+TAT)
#### ---- Polynomial Regression ---- ####
fit_poly =  lm(CO~TIT, data = ds[train_set, ]);
summary(fit_poly)
#### ---- Polynomial Regression ---- ####
fit_poly =  lm(CO~poly(TIT,2), data = ds[train_set, ]);
summary(fit_poly)
fit_poly_3 =  lm(CO~poly(TIT,3), data = ds[train_set, ]);
summary(fit_poly_3)
# Osserviamo come R^2 solo con poly aumenta a 0.60
summary(fit_poly)
summary(fit_poly_3)
dev.new()
plot(TIT, CO)
x <- with(ds[train_set, ], seq(min(ds[train_set, ]$TIT), max(ds[train_set, ]$TIT), length.out=2000))
y <- predict(fit_poly, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
x <- with(ds[train_set, ], seq(min(ds[train_set, ]$TIT), max(ds[train_set, ]$TIT), length.out=2000))
y <- predict(fit_poly_3, newdata = data.frame(TIT = x))
lines(x, y, col = "blue")
fit_poly_4 =  lm(CO~poly(TIT,4), data = ds[train_set, ]);
summary(fit_poly_4)
x <- with(ds[train_set, ], seq(min(ds[train_set, ]$TIT), max(ds[train_set, ]$TIT), length.out=2000))
y <- predict(fit_poly_4, newdata = data.frame(TIT = x))
lines(x, y, col = "yellow")
#### ---- Fit1 + poly ---- ####
fit1.poly2 = lm(CO~.-NOX-TEY-CDP-GTEP+poly(TIT, 2), data = ds[train_set, ]);
fit1.poly3 = lm(CO~.-NOX-TEY-CDP-GTEP+poly(TIT, 3), data = ds[train_set, ]);
summary(fit1.poly2)
#### ---- Fit1 + poly ---- ####
fit1.poly2 = lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 2), data = ds[train_set, ]);
fit1.poly3 = lm(CO~.-NOX-TEY-CDP-GTEP+poly(TIT, 3), data = ds[train_set, ]);
summary(fit1.poly2)
fit1.poly3 = lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 3), data = ds[train_set, ]);
summary(fit1.poly3)
vif(fit1.poly2)
vif(fit1.poly3)
dev.new()
par(mfrow = c(2,2))
plot(fit1.poly2)
#### ---- Outliears and High leverage point removing---- ####
w <- abs(rstudent(fit1.poly2)) < 3 & abs(cooks.distance(fit1.poly2)) < 4/nrow(fit1.poly2$model)
ds_train = ds[train_set,]
ds_train_no_out = ds_train[w,]
nrow(ds_train)
nrow(ds_train_no_out)
fit1.poly2_no_out <- lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 2), data = ds_train_no_out);
summary(fit1.poly2_no_out)
fit1.poly3_no_out <- lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 3), data = ds_train_no_out);
summary(fit1.poly3_no_out)
dev.new()
par(mfrow = c(2,2))
plot(fit1.poly2_no_out)
dev.new()
par(mfrow = c(2,2))
plot(fit1.poly3_no_out)
fit2 <- lm(CO~.-NOX-TEY-CDP-GTEP, data = ds_train_no_out);
summary(fit2)
summary(fit1.poly2_no_out)
summary(fit2)
dev.new()
par(mfrow = c(2,2))
plot(fit2)
dev.new()
par(mfrow = c(2,2))
plot(fit1.poly3)
# Il modello senza i regressori di sopra ha un valore di R^2 paragonabile al modello totale,
# a dimostrazione del fatto che l'informazione portata da TEY,CDP e GTEP può essere spiegata da altri regressori
# e la F-statistic aumenta.
summary(fit1)
anova(fit1.poly2, fit1.poly2)
anova(fit1.poly2, fit1.poly3)
#### ---- Outliears and High leverage point removing---- ####
# In questa sezione andiamo ad eliminare quei punti che risultano essere outliers e di high le
w <- abs(rstudent(fit1.poly2)) < 3 | abs(cooks.distance(fit1.poly2)) < 4/nrow(fit1.poly2$model)
ds_train = ds[train_set,]
ds_train_no_out = ds_train[w,]
fit2 <- lm(CO~.-NOX-TEY-CDP-GTEP, data = ds_train_no_out);
summary(fit2)
fit1.poly2_no_out <- lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 2), data = ds_train_no_out);
summary(fit1.poly2_no_out)
fit1.poly3_no_out <- lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 3), data = ds_train_no_out);
summary(fit1.poly3_no_out)
dev.new()
par(mfrow = c(2,2))
plot(fit1.poly2_no_out)
w <- abs(rstudent(fit1.poly2)) < 3 & abs(cooks.distance(fit1.poly2)) < 4/nrow(fit1.poly2$model)
ds_train = ds[train_set,]
ds_train_no_out = ds_train[w,]
# A causa della eliminazione dei punti con valori insoliti, si osserva un generale aumento della
# variabilità spiegata,
fit2 <- lm(CO~.-NOX-TEY-CDP-GTEP, data = ds_train_no_out);
summary(fit2)
fit1.poly2_no_out <- lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 2), data = ds_train_no_out);
summary(fit1.poly2_no_out)
fit1.poly3_no_out <- lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 3), data = ds_train_no_out);
summary(fit1.poly3_no_out)
#### ---- Poly(AFDP) ---- ####
fit1.poly2 = lm(CO~poly(AFDP, 2), data = ds[train_set, ]);
#### ---- Poly(AFDP) ---- ####
fit1.poly2_afdp = lm(CO~poly(AFDP, 2), data = ds[train_set, ]);
summary(fit1.poly2_afdp)
dev.new()
plot(CO, AFDP)
plot(AFDP, CO)
x <- with(ds[train_set, ], seq(min(ds[train_set, ]$AFDP), max(ds[train_set, ]$AFDP), length.out=2000))
y <- predict(fit1.poly2, newdata = data.frame(TIT = x))
y <- predict(fit1.poly2, newdata = data.frame(AFDP = x))
lines(x, y, col = "red")
#### ---- Poly(AFDP) ---- ####
fit1.poly2_afdp = lm(CO~AFDP, data = ds[train_set, ]);
summary(fit1.poly2_afdp)
#### ---- Poly(AFDP) ---- ####
fit1_afdp = lm(CO~AFDP, data = ds[train_set, ]);
fit1.poly2_afdp = lm(CO~poly(AFDP, 2), data = ds[train_set, ]);
summary(fit1.poly2_afdp)
fit1.poly2_tit_afdp = lm(CO~.-NOX-TEY-CDP-GTEP-TIT-AFDP+poly(TIT, 2)+poly(AFDP, 2), data = ds[train_set, ]);
summary(fit1.poly2_tit_afdp)
set.seed(1)
#### ---- Models evaluation ---- ####
# Si andranno a testare i seguenti modelli:
# fit_all, fit1, fit1.poly2, fit1.poly3, fit2, fit1.poly2_no_out, fit1.poly3_no_out
fit_all_val_mse = mean((ds$CO[validation_set]-predict(fit_all,ds[validation_set]))^2)
# Partiamo con l'eseguire un fit con tutti i regressori, al fine di verficare che
# 1. In presenza di regressori correlati il VIF è elevato;
# 2. Andando a selezionare le feature realmente dipendenti il MSE di test stimato diminuisce
fit_all = lm(CO~.-NOX, data = ds[train_set, ])
#### ---- Models evaluation ---- ####
# Si andranno a testare i seguenti modelli:
# fit_all, fit1, fit1.poly2, fit1.poly3, fit2, fit1.poly2_no_out, fit1.poly3_no_out
fit_all_val_mse = mean((ds$CO[validation_set]-predict(fit_all,ds[validation_set]))^2)
#### ---- Models evaluation ---- ####
# Si andranno a testare i seguenti modelli:
# fit_all, fit1, fit1.poly2, fit1.poly3, fit2, fit1.poly2_no_out, fit1.poly3_no_out
fit_all_val_mse = mean((ds$CO[validation_set]-predict(fit_all,ds[validation_se,t]))^2)
#### ---- Models evaluation ---- ####
# Si andranno a testare i seguenti modelli:
# fit_all, fit1, fit1.poly2, fit1.poly3, fit2, fit1.poly2_no_out, fit1.poly3_no_out
fit_all_val_mse = mean((ds$CO[validation_set]-predict(fit_all,ds[validation_set, ]))^2)
fit_all_val_mse
# Eliminiamo quei regressori con un VIF elevato
fit1 = lm(CO~.-NOX-TEY-CDP-GTEP, data = ds[train_set, ]);
#### ---- Fit1 + poly(TIT) ---- ####
fit1.poly2 = lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 2), data = ds[train_set, ]);
fit1.poly3 = lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 3), data = ds[train_set, ]);
# A causa della eliminazione dei punti con valori insoliti, si osserva un generale aumento della
# variabilità spiegata, fit2 -> r2 = 0.67; poly2 = 0.7544; poly3 =  0.7574.
fit2 <- lm(CO~.-NOX-TEY-CDP-GTEP, data = ds_train_no_out);
fit1.poly2_no_out <- lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 2), data = ds_train_no_out);
fit1.poly3_no_out <- lm(CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 3), data = ds_train_no_out);
fit_all_val_mse = mean((ds$CO[validation_set]-predict(fit_all,ds[validation_set, ]))^2)
fit_1_val_mse = mean((ds$CO[validation_set]-predict(fit1,ds[validation_set, ]))^2)
fit_1_poly2_val_mse = mean((ds$CO[validation_set]-predict(fit1.poly2,ds[validation_set, ]))^2)
fit_1_poly3_val_mse = mean((ds$CO[validation_set]-predict(fit1.poly3,ds[validation_set, ]))^2)
fit_2 = mean((ds$CO[validation_set]-predict(fit2,ds[validation_set, ]))^2)
fit_1_poly2_no_out_val_mse = mean((ds$CO[validation_set]-predict(fit1.poly2_no_out,ds[validation_set, ]))^2)
fit_1_poly3_no_out_val_mse = mean((ds$CO[validation_set]-predict(fit1.poly3_no_out,ds[validation_set, ]))^2)
fit_all_val_mse
fit_1_val_mse
fit_1_poly2_val_mse
fit_1_poly3_val_mse
fit_2
fit_1_poly2_no_out_val_mse
fit_1_poly3_no_out_val_mse
fit_all_val_mse = mean((ds$CO[validation_set]-predict(fit_all,ds[validation_set, ]))^2)
fit_1_val_mse = mean((ds$CO[validation_set]-predict(fit1,ds[validation_set, ]))^2)
fit_1_poly2_val_mse = mean((ds$CO[validation_set]-predict(fit1.poly2,ds[validation_set, ]))^2)
fit_1_poly3_val_mse = mean((ds$CO[validation_set]-predict(fit1.poly3,ds[validation_set, ]))^2)
fit_2 = mean((ds$CO[validation_set]-predict(fit2,ds[validation_set, ]))^2)
fit_1_poly2_no_out_val_mse = mean((ds$CO[validation_set]-predict(fit1.poly2_no_out,ds[validation_set, ]))^2)
fit_1_poly3_no_out_val_mse = mean((ds$CO[validation_set]-predict(fit1.poly3_no_out,ds[validation_set, ]))^2)
fit_all_val_mse
fit_1_val_mse
fit_1_poly2_val_mse
fit_1_poly3_val_mse
fit_2
fit_1_poly2_no_out_val_mse
fit_1_poly3_no_out_val_mse
summary(CO)
mean((ds$CO[train_set]-predict(fit_all,ds[train_set, ]))^2) # 2.36
fit_1_val_mse = mean((ds$CO[validation_set]-predict(fit1,ds[validation_set, ]))^2) # 2.40
mean((ds$CO[train_set]-predict(fit1,ds[train_set, ]))^2) # 2.40
hyst(CO)
hist(CO)
fit1.poly2$coefficients
fit1$coefficients
#### ---- Polynomial Regression ---- ####
fit_poly =  lm(CO~poly(TIT,2), data = ds[train_set, ]);
fit_poly_val_mse = mean((ds$CO[validation_set]-predict(fit_poly_val_mse,ds[validation_set, ]))^2) # 2.40
fit_poly_val_mse = mean((ds$CO[validation_set]-predict(fit_poly,ds[validation_set, ]))^2) # 2.40
fit_poly_val_mse
# Dal sumary si
summary(fit1.poly2)
fit1.poly2_omo = lm(log(CO)~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 2), data = ds[train_set, ]);
log(C=)
log(CO)
summary(fit1.poly2_omo)
dev.new()
par(mfrow = c(2,2))
plot(fit1.poly2_omo)
fit1.poly2_omo = lm(sqrt(CO)~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 2), data = ds[train_set, ]);
summary(fit1.poly2_omo)
dev.new()
par(mfrow = c(2,2))
plot(fit1.poly2_omo)
fit1.poly2_omo = lm(CO^2~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 2), data = ds[train_set, ]);
summary(fit1.poly2_omo)
dev.new()
train = (nrow(ds)*70)/100
test = (nrow(ds)*30)/100
set.seed(1)
training_set = sample(x, train, rep = FALSE)
x = c(1:nrow(ds))
train = (nrow(ds)*70)/100
test = (nrow(ds)*30)/100
set.seed(1)
training_set = sample(x, train, rep = FALSE)
test_set = sample(x[-train_set], validation, rep = FALSE)
confint(fit_all)
xx <- seq(min(CO),max(CO),along.with = CO)
ci_non_lin <- predict(fit_all, newdata = data.frame(CO=xx), se.fit = T, interval = 'confidence')
ci_non_lin
confint(fit_all)
confint(fit1)
coef(fit1)
coef(fit_all)
#### ---- Polynomial Regression ---- ####
fit_tit =  lm(CO~TIT, data = ds[train_set, ]);
summary(fit_tit)
# Osserviamo come R^2 solo con poly aumenta a 0.60
summary(fit_poly)
ci_pred_TIT = predict(fit_tit, newdata = data.frame(CO = xx), se.fit = T, interval = "confidence")
pi_pred_TIT = predict(fit_tit, newdata = data.frame(CO = xx), se.fit = T, interval = "confidence")
summary(fit1.poly2)
plot(TIT, CO)
dev.new()
plot(TIT, CO)
abline(fit_tit)
dev.new()
plot(TIT, CO)
abline(fit_tit, col = "blue")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T)
pi_pred_TIT = predict(fit_tit, newdata = data.frame(CO = xx), se.fit = T, interval = "confidence")
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T)
pi_pred_TIT = predict(fit_tit, newdata = data.frame(CO = xx), set.fit = T, interval = "prediction")
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T)
dev.new()
plot(TIT, CO)
abline(fit_tit, col = "blue")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T)
matplot
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T)
ci_pred_TIT
ci_pred_TIT = predict(fit_tit, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT = predict(fit_tit, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
dev.new()
plot(TIT, CO)
abline(fit_tit, col = "blue")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T)
ci_pred_TIT = predict(fit_tit, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT = predict(fit_tit, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
dev.new()
plot(TIT, CO)
abline(fit_tit, col = "blue")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T)
#### ---- How confidence and prediction intervals change ---- ####
# Andiamo a confrontare come gli intervalli di confidenza e predizione sulla risposta CO, rispetto al regressore
# TIT, per il quale è stato osservato un legame con CO, cambiano rispetto alle varie modifiche apportate al modello
# con l'aspettativa che:
# 1. Introducendo il termine quadratico, l'intervallo si restringa, rispetto alla semplice regressione lineare;
# 2. Intervalli non si modificano con l'introduzione degli altri regressori, in quanto è stato osservato come in realtà a dominare
# è il termine polinomiale
xx <- seq(min(TIT), max(TIT), along.with = TIT)
ci_pred_TIT = predict(fit_tit, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT = predict(fit_tit, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
dev.new()
plot(TIT, CO)
abline(fit_tit, col = "blue")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T)
library(MASS)
# il primo tentativo che facciamo e predire medv con il regressore lstat
attach(Boston)
# quello che noi stiamo facendo adesso e plottare il valore di medv in funzione di lstat, tenendo conto anche dell'influenza
# degli altri regressori su medv
# quello che faccimo e dire che il nostro legame principale rimane medv -> lstat.
# quindi ci domandiamo come l'influenza degli altri regressori fanno variare il nostro legame principale.
# quello che possiamo osservare che effettivamente la presenza degli altri regressori influenzano il nostro legame principale.
# questo e un modo per capire la variabilita introdotto dagli altri regressori.
xx <- seq(min(lstat), max(lstat), along.with = lstat)
ci_multre <- predict(multre, newdata = data.frame(lstat = xx), se.fit = T, interval = "confidence")
dev.new()
par(mfrow = c(1,1))
plot(lstat, medv, ylim = c(0,60))
matplot(xx, ci_multre$fit[,1], lty = 1, ltw = 2, col = "red",type = "l", add = T)
matplot(xx, ci_multre$fit[,2], lty = 2, ltw = 2, col = "red",type = "l", add = T)
matplot(xx, ci_multre$fit[,3], lty = 2, ltw = 2, col = "red",type = "l", add = T)
ci_multre <- predict(multre, newdata = data.frame(lstat = xx), se.fit = T, interval = "confidence")
plot(lstat, medv, ylim = c(0,60))
matplot(xx, ci_multre$fit[,1], lty = 1, ltw = 2, col = "red",type = "l", add = T)
matplot(xx, ci_multre$fit[,2], lty = 2, ltw = 2, col = "red",type = "l", add = T)
matplot(xx, ci_multre$fit[,3], lty = 2, ltw = 2, col = "red",type = "l", add = T)
#### ---- Poly TIT only ---- ####
ci_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T)
matplot(xx, ci_pred_TIT_poly$fit[,3],,lty=3,col="yellow",type="l",add = T)
matplot(xx, pi_pred_TIT_poly[,2],,lty=3, col="violet", type = "l", add = T)
matplot(xx, pi_pred_TIT_poly[,3],,lty=3, col="violet", type = "l", add = T)
xx <- seq(min(TIT), max(TIT), along.with = TIT)
ci_pred_TIT = predict(fit_tit, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT = predict(fit_tit, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
dev.new()
plot(TIT, CO)
#### ---- Linear only TIT ---- ####
abline(fit_tit, col = "blue")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T)
#### ---- Linear TIT with others ---- ####
#### ---- Poly TIT only ---- ####
ci_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T)
matplot(xx, ci_pred_TIT_poly$fit[,3],,lty=3,col="yellow",type="l",add = T)
matplot(xx, pi_pred_TIT_poly[,2],,lty=3, col="violet", type = "l", add = T)
matplot(xx, pi_pred_TIT_poly[,3],,lty=3, col="violet", type = "l", add = T)
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 2)
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 100)
dev.new()
plot(TIT, CO)
#### ---- Linear only TIT ---- ####
abline(fit_tit, col = "blue")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T)
#### ---- Poly TIT only ---- ####
ci_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 10)
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 3)
dev.new()
plot(TIT, CO)
#### ---- Linear only TIT ---- ####
abline(fit_tit, col = "blue")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T)
#### ---- Poly TIT only ---- ####
ci_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 3)
#### ---- Poly TIT only ---- ####
y <- predict(fit_poly, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
matplot(xx, ci_pred_TIT_poly$fit[,3],,lty=3,col="yellow",type="l",add = T, lwd = 3)
matplot(xx, pi_pred_TIT_poly[,2],,lty=3, col="violet", type = "l", add = T, lwd = 3)
matplot(xx, pi_pred_TIT_poly[,3],,lty=3, col="violet", type = "l", add = T, lwd = 3)
