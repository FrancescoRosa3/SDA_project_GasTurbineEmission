ridge.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x[test_set, ])
lasso.pred_mse = mean((lasso.pred-y[test_set])^2) #  2.402396
ridge.pred_mse = mean((ridge.pred-y[test_set])^2) #  2.515268
lasso.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x[training_set, ])
ridge.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x[training_set, ])
lasso.pred_mes_tr = mean((lasso.pred-y[training_set])^2) # 2.389104
ridge.pred_mes_tr = mean((ridge.pred-y[training_set])^2) # 2.492085
#### ---- Introduzione del polinomio ---- ####
# Fissata la presenza di una relazione non lineare tra i regressori e l'uscita
# si andrà a provare il ridge e lasso shrinkage method sul modello che introduce anche
# il polinomio sulla variabile TIT, aspettandoci di osservare un comportamento simile
# a quello di sopra, quindi con un fattore di regolarizzazione basso per Ridge, in quanto
# dalla analisi sul least square già si poteva osservare come il polinomio dominava gli altri
# coefficienti che erano molto vicino a zero, nel caso del Lasso, ci aspettiamo la selezione del solo polinomio
# dato comunque un fattore di smorzamento basso
x  = model.matrix(CO~.-NOX-TIT+poly(TIT,3), ds)[, -1]
y = ds$CO
xx = c(1:nrow(ds))
train = (nrow(ds)*70)/100
test = (nrow(ds)*30)/100
set.seed(1)
training_set = sample(xx, train, rep = FALSE)
test_set = sample(xx[-training_set], test, rep = FALSE)
#### ---- Ridge + Polinomio ---- ####
# K = 10
cvridge_poly.mod = cv.glmnet(x[training_set, ], y[training_set], alpha = 0, lambda = grid)
# come era lecito aspettarsi il lambda migliore si trova per valori del log(lambda) negativi
# cioè per valori di lambda vicini allo zero
plot(cvridge.mod)
bestlambda_ridge_poly = cvridge_poly.mod$lambda.min
lambda1SE_ridge_poly = cvridge_poly.mod$lambda.1se
# Coefficienti praticamente uguali a quelli del modello lineare
coef(cvridge_poly.mod, bestlambda_ridge_poly) # coefficienti del modello per il lambda migliore
ridge.mod = glmnet(x[training_set, ], y[training_set], alpha = 0, lambda = grid)
dev.new()
plot(ridge.mod, label = T, xvar = "lambda")
#### ---- Lasso + Polinomio ---- ####
# K = 10
cvlasso_poly.mod = cv.glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid)
# Osserviamo come il lambda migliore si ottiene per valori piccoli, interessante è osservare
# il lambda+1SE, infatti per quel valore di lambda otteniamo prestazioni sul test set paragonabili
# ma con un modello con soli 3 regressori
plot(cvlasso_poly.mod)
bestlambda_lasso_poly = cvlasso_poly.mod$lambda.min
lambda1SE_lasso_poly = cvlasso_poly.mod$lambda.1se
# Osserviamo come i coefficienti diversi da zero sono proprio quelli legati al polinomio
coef(cvlasso_poly.mod, lambda1SE_lasso_poly)
lasso.mod = glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid)
dev.new()
plot(lasso.mod, label = T, xvar = "lambda")
#### ---- Best Lasso+Poly vs Best Ridge+Poly ---- ####
# Osserviamo ancora come con l'aggiunta del polinomio ci permette di ottenere ancora
# un lieve miglioramento delle performance a giustificazione ancora del fatto come
# il regressore TIT è fortemente legato all'uscita e come questo è legato in maniera non lineare
# alla stessa.
lasso_poly.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x[test_set, ])
ridge_poly.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x[test_set, ])
lasso_poly.pred_mse = mean((lasso_poly.pred-y[test_set])^2) #  2.06969
ridge_poly.pred_mse = mean((ridge_poly.pred-y[test_set])^2) #  2.126376
#### ---- Library ---- ####
library(glmnet)
x  = model.matrix(CO~.-NOX, ds)[, -1]
y = ds$CO
# Vettore dei coefficienti lambda
grid = 10^seq(10,-2, length = 100)
# Dividiamo quindi il data set in 2 set.
xx = c(1:nrow(ds))
train = (nrow(ds)*70)/100
test = (nrow(ds)*30)/100
# Sul training andiamo ad eseguire la CV per individuare il lambda miglire
# Sul test andremmo a valutare il modello ottenuto per fare un confronto Ridge vs Lasso
set.seed(1)
training_set = sample(xx, train, rep = FALSE)
test_set = sample(xx[-training_set], test, rep = FALSE)
#### ---- Ridge Regression ---- ####
# K = 10
cvridge.mod = cv.glmnet(x[training_set, ], y[training_set], alpha = 0, lambda = grid)
# Osserviamo come all'aumentare del log(lamda) quindi all'aumentare di lamda aumenta il MSE di test predetto
# questo significa che andando a ridurre l'importanza di alcuni regressori si ottiene una capacità di predizione
# peggiore
plot(cvridge.mod)
# Osserviamo come all'aumentare del log(lamda) quindi all'aumentare di lamda aumenta il MSE di test predetto
# questo significa che andando a ridurre l'importanza di alcuni regressori si ottiene una capacità di predizione
# peggiore
dev.new()
plot(cvridge.mod)
# Osserviamo come il lambda a 1se è pari a 0.49,
# ancora capiamo che l'influenza dell'indice di regolarizzazione è praticamente inutile
bestlambda_ridge = cvridge.mod$lambda.min
lambda1SE_ridge = cvridge.mod$lambda.1se
# Il miglior lambda si ottiene su 0.01, praticamente pari a 0, cioè pari alla regressione least square classica
# Ciò si potrebbe giustificare con il fatto che i coefficienti del modello least square con tutti i regressori
# sono già relativamente bassi.
lm_all = lm(CO~.-NOX, data = ds[training_set, ])
coef(lm_all) # coefficienti modello Least Square
coef(cvridge.mod, bestlambda) # coefficienti del modello per il lambda migliore
coef(cvridge.mod, bestlambda_ridge) # coefficienti del modello per il lambda migliore
# Osserviamo come i coefficienti partono da valori piccoli, quindi all'aumentare del lambda si vanno
# a stringere.
ridge.mod = glmnet(x[training_set, ], y[training_set], alpha = 0, lambda = grid)
dev.new()
plot(ridge.mod, label = T, xvar = "lambda")
#### ---- Lasso ---- ####
# Si osserva come il fattore di regolarizzazione lambda rimane piccolo
# però adesso si può osservare la capacità di effettuare variable selection
# della tecnica Lasso, infatti osserviamo come in corrispondenza di lambda+1SE
# i coefficienti irrilevanti si portano a 0, mantenendo diversi da 0 solo
# TIT e TAT, andando a confermare le intuizioni fatte in least_square.R
cvlasso.mod = cv.glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid, thresh = 1e-12)
plot(cvlasso.mod)
bestlambda_lasso = cvlasso.mod$lambda.min
lambda1SE_lasso = cvlasso.mod$lambda.1se
coef(cvlasso.mod, bestlambda) # coefficienti del modello per il lambda migliore
coef(cvlasso.mod, lambda1SE_lasso) # coefficienti del modello per il lambda + 1SE
lasso.mod = glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid)
dev.new()
plot(lasso.mod, label = T, xvar = "lambda")
dev.new()
plot(cvlasso.mod)
bestlambda_lasso = cvlasso.mod$lambda.min
lambda1SE_lasso = cvlasso.mod$lambda.1se
coef(cvlasso.mod, bestlambda) # coefficienti del modello per il lambda migliore
coef(cvlasso.mod, lambda1SE_lasso) # coefficienti del modello per il lambda + 1SE
lasso.mod = glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid)
dev.new()
plot(lasso.mod, label = T, xvar = "lambda")
#### ---- Best Lasso vs Best Ridge ---- ####
# Andiamo a confrontare il Best Lasso (praticamente uguale al Least Square) e il Best Ridge
# al fine di valutare le prestazioni di predizione sul Test Set
# Osserviamo come le performance tra Ridge e Lasso sono tra loro paragonabili
# anche se il modello lasso con solo due coefficienti riesce ad avere prestazioni migliori.
lasso.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x[test_set, ])
ridge.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x[test_set, ])
lasso.pred_mse = mean((lasso.pred-y[test_set])^2) #  2.402396
#### ---- Best Lasso vs Best Ridge ---- ####
# Andiamo a confrontare il Best Lasso (praticamente uguale al Least Square) e il Best Ridge
# al fine di valutare le prestazioni di predizione sul Test Set
# Osserviamo come le performance tra Ridge e Lasso sono tra loro paragonabili
# anche se il modello lasso con solo due coefficienti riesce ad avere prestazioni migliori.
lasso.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x[test_set, ])
ridge.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x[test_set, ])
lasso.pred_mse = mean((lasso.pred-y[test_set])^2) #  2.402396
ridge.pred_mse = mean((ridge.pred-y[test_set])^2) #  2.515268
lasso.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x[training_set, ])
ridge.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x[training_set, ])
lasso.pred_mes_tr = mean((lasso.pred-y[training_set])^2) # 2.389104
ridge.pred_mes_tr = mean((ridge.pred-y[training_set])^2) # 2.492085
#### ---- Introduzione del polinomio ---- ####
# Fissata la presenza di una relazione non lineare tra i regressori e l'uscita
# si andrà a provare il ridge e lasso shrinkage method sul modello che introduce anche
# il polinomio sulla variabile TIT, aspettandoci di osservare un comportamento simile
# a quello di sopra, quindi con un fattore di regolarizzazione basso per Ridge, in quanto
# dalla analisi sul least square già si poteva osservare come il polinomio dominava gli altri
# coefficienti che erano molto vicino a zero, nel caso del Lasso, ci aspettiamo la selezione del solo polinomio
# dato comunque un fattore di smorzamento basso
x  = model.matrix(CO~.-NOX-TIT+poly(TIT,3), ds)[, -1]
y = ds$CO
xx = c(1:nrow(ds))
train = (nrow(ds)*70)/100
test = (nrow(ds)*30)/100
set.seed(1)
training_set = sample(xx, train, rep = FALSE)
test_set = sample(xx[-training_set], test, rep = FALSE)
#### ---- Ridge + Polinomio ---- ####
# K = 10
cvridge_poly.mod = cv.glmnet(x[training_set, ], y[training_set], alpha = 0, lambda = grid)
# come era lecito aspettarsi il lambda migliore si trova per valori del log(lambda) negativi
# cioè per valori di lambda vicini allo zero
dev.new()
plot(cvridge.mod)
plot(cvridge_poly.mod)
bestlambda_ridge_poly = cvridge_poly.mod$lambda.min
lambda1SE_ridge_poly = cvridge_poly.mod$lambda.1se
# Coefficienti praticamente uguali a quelli del modello lineare
coef(cvridge_poly.mod, bestlambda_ridge_poly) # coefficienti del modello per il lambda migliore
ridge.mod = glmnet(x[training_set, ], y[training_set], alpha = 0, lambda = grid)
dev.new()
plot(ridge.mod, label = T, xvar = "lambda")
#### ---- Lasso + Polinomio ---- ####
# K = 10
cvlasso_poly.mod = cv.glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid)
# Osserviamo come il lambda migliore si ottiene per valori piccoli, interessante è osservare
# il lambda+1SE, infatti per quel valore di lambda otteniamo prestazioni sul test set paragonabili
# ma con un modello con soli 3 regressori
plot(cvlasso_poly.mod)
# Osserviamo come il lambda migliore si ottiene per valori piccoli, interessante è osservare
# il lambda+1SE, infatti per quel valore di lambda otteniamo prestazioni sul test set paragonabili
# ma con un modello con soli 3 regressori
dev.new()
plot(cvlasso_poly.mod)
bestlambda_lasso_poly = cvlasso_poly.mod$lambda.min
bestlambda_lasso_poly
lambda1SE_lasso_poly
# Osserviamo come i coefficienti diversi da zero sono proprio quelli legati al polinomio
coef(cvlasso_poly.mod, lambda1SE_lasso_poly)
lasso.mod = glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid)
dev.new()
plot(lasso.mod, label = T, xvar = "lambda")
#### ---- Best Lasso+Poly vs Best Ridge+Poly ---- ####
# Osserviamo ancora come con l'aggiunta del polinomio ci permette di ottenere ancora
# un lieve miglioramento delle performance a giustificazione ancora del fatto come
# il regressore TIT è fortemente legato all'uscita e come questo è legato in maniera non lineare
# alla stessa.
lasso_poly.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x[test_set, ])
ridge_poly.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x[test_set, ])
lasso_poly.pred_mse = mean((lasso_poly.pred-y[test_set])^2) #  2.06969
ridge_poly.pred_mse = mean((ridge_poly.pred-y[test_set])^2) #  2.126376
lasso_poly.pred_mse
ridge_poly.pred_mse
# Sul training andiamo ad eseguire la CV per individuare il lambda miglire
# Sul test andremmo a valutare il modello ottenuto per fare un confronto Ridge vs Lasso
set.seed(1)
training_set = sample(xx, train, rep = FALSE)
test_set = sample(xx[-training_set], test, rep = FALSE)
#### ---- Shrinkage Method ---- ####
# Gli schrinkage methods (Ridge e Lasso) vengono applicati in un contesto in cui
# p << n e dove il dataset presenta problemi di multicollinearità.
# Questo significa che in generale i metodi di shrinkage dovrebbero darci prestazioni migliori
# rispetto al fit Least Square che comprende tutti i regressori.
#### ---- Library ---- ####
library(glmnet)
x  = model.matrix(CO~.-NOX, ds)[, -1]
y = ds$CO
# Vettore dei coefficienti lambda
grid = 10^seq(10,-2, length = 100)
# Sia la tecnica ridge che la tecnica lasso hanno un coefficiente lambda da determinare
# la tecnica utilizzata per la stima del lambda migliore è la Cross Validation.
# Dividiamo quindi il data set in 2 set.
xx = c(1:nrow(ds))
train = (nrow(ds)*70)/100
test = (nrow(ds)*30)/100
# Sul training andiamo ad eseguire la CV per individuare il lambda miglire
# Sul test andremmo a valutare il modello ottenuto per fare un confronto Ridge vs Lasso
set.seed(1)
training_set = sample(xx, train, rep = FALSE)
test_set = sample(xx[-training_set], test, rep = FALSE)
#### ---- Ridge Regression ---- ####
# K = 10
cvridge.mod = cv.glmnet(x[training_set, ], y[training_set], alpha = 0, lambda = grid)
# Osserviamo come all'aumentare del log(lamda) quindi all'aumentare di lamda aumenta il MSE di test predetto
# questo significa che andando a ridurre l'importanza di alcuni regressori si ottiene una capacità di predizione
# peggiore
dev.new()
plot(cvridge.mod)
# Osserviamo come il lambda a 1se è pari a 0.49,
# ancora capiamo che l'influenza dell'indice di regolarizzazione è praticamente inutile
bestlambda_ridge = cvridge.mod$lambda.min
lambda1SE_ridge = cvridge.mod$lambda.1se
# Il miglior lambda si ottiene su 0.01, praticamente pari a 0, cioè pari alla regressione least square classica
# Ciò si potrebbe giustificare con il fatto che i coefficienti del modello least square con tutti i regressori
# sono già relativamente bassi.
lm_all = lm(CO~.-NOX, data = ds[training_set, ])
coef(lm_all) # coefficienti modello Least Square
coef(cvridge.mod, bestlambda) # coefficienti del modello per il lambda migliore
# Osserviamo come i coefficienti partono da valori piccoli, quindi all'aumentare del lambda si vanno
# a stringere.
ridge.mod = glmnet(x[training_set, ], y[training_set], alpha = 0, lambda = grid)
dev.new()
plot(ridge.mod, label = T, xvar = "lambda")
#### ---- Lasso ---- ####
# Si osserva come il fattore di regolarizzazione lambda rimane piccolo
# però adesso si può osservare la capacità di effettuare variable selection
# della tecnica Lasso, infatti osserviamo come in corrispondenza di lambda+1SE
# i coefficienti irrilevanti si portano a 0, mantenendo diversi da 0 solo
# TIT e TAT, andando a confermare le intuizioni fatte in least_square.R
cvlasso.mod = cv.glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid, thresh = 1e-12)
dev.new()
plot(cvlasso.mod)
bestlambda_lasso = cvlasso.mod$lambda.min
lambda1SE_lasso = cvlasso.mod$lambda.1se
coef(cvlasso.mod, bestlambda_lasso) # coefficienti del modello per il lambda migliore
coef(cvlasso.mod, lambda1SE_lasso) # coefficienti del modello per il lambda + 1SE
lasso.mod = glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid)
dev.new()
plot(lasso.mod, label = T, xvar = "lambda")
#### ---- Best Lasso vs Best Ridge ---- ####
# Andiamo a confrontare il Best Lasso (praticamente uguale al Least Square) e il Best Ridge
# al fine di valutare le prestazioni di predizione sul Test Set
# Osserviamo come le performance tra Ridge e Lasso sono tra loro paragonabili
# anche se il modello lasso con solo due coefficienti riesce ad avere prestazioni migliori.
lasso.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x[test_set, ])
ridge.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x[test_set, ])
lasso.pred_mse = mean((lasso.pred-y[test_set])^2) #  2.402396
ridge.pred_mse = mean((ridge.pred-y[test_set])^2) #  2.515268
lasso.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x[training_set, ])
ridge.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x[training_set, ])
lasso.pred_mes_tr = mean((lasso.pred-y[training_set])^2) # 2.389104
ridge.pred_mes_tr = mean((ridge.pred-y[training_set])^2) # 2.492085
#### ---- Introduzione del polinomio ---- ####
# Fissata la presenza di una relazione non lineare tra i regressori e l'uscita
# si andrà a provare il ridge e lasso shrinkage method sul modello che introduce anche
# il polinomio sulla variabile TIT, aspettandoci di osservare un comportamento simile
# a quello di sopra, quindi con un fattore di regolarizzazione basso per Ridge, in quanto
# dalla analisi sul least square già si poteva osservare come il polinomio dominava gli altri
# coefficienti che erano molto vicino a zero, nel caso del Lasso, ci aspettiamo la selezione del solo polinomio
# dato comunque un fattore di smorzamento basso
x  = model.matrix(CO~.-NOX-TIT+poly(TIT,3), ds)[, -1]
y = ds$CO
xx = c(1:nrow(ds))
train = (nrow(ds)*70)/100
test = (nrow(ds)*30)/100
set.seed(1)
training_set = sample(xx, train, rep = FALSE)
test_set = sample(xx[-training_set], test, rep = FALSE)
#### ---- Ridge + Polinomio ---- ####
# K = 10
cvridge_poly.mod = cv.glmnet(x[training_set, ], y[training_set], alpha = 0, lambda = grid)
# come era lecito aspettarsi il lambda migliore si trova per valori del log(lambda) negativi
# cioè per valori di lambda vicini allo zero
dev.new()
plot(cvridge_poly.mod)
bestlambda_ridge_poly = cvridge_poly.mod$lambda.min
lambda1SE_ridge_poly = cvridge_poly.mod$lambda.1se
# Coefficienti praticamente uguali a quelli del modello lineare
coef(cvridge_poly.mod, bestlambda_ridge_poly) # coefficienti del modello per il lambda migliore
ridge.mod = glmnet(x[training_set, ], y[training_set], alpha = 0, lambda = grid)
dev.new()
plot(ridge.mod, label = T, xvar = "lambda")
#### ---- Lasso + Polinomio ---- ####
# K = 10
cvlasso_poly.mod = cv.glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid)
# Osserviamo come il lambda migliore si ottiene per valori piccoli, interessante è osservare
# il lambda+1SE, infatti per quel valore di lambda otteniamo prestazioni sul test set paragonabili
# ma con un modello con soli 3 regressori
dev.new()
plot(cvlasso_poly.mod)
bestlambda_lasso_poly = cvlasso_poly.mod$lambda.min
lambda1SE_lasso_poly = cvlasso_poly.mod$lambda.1se
# Osserviamo come i coefficienti diversi da zero sono proprio quelli legati al polinomio
coef(cvlasso_poly.mod, lambda1SE_lasso_poly)
lasso.mod = glmnet(x[training_set, ], y[training_set], alpha = 1, lambda = grid)
dev.new()
plot(lasso.mod, label = T, xvar = "lambda")
#### ---- Best Lasso+Poly vs Best Ridge+Poly ---- ####
# Osserviamo ancora come con l'aggiunta del polinomio ci permette di ottenere ancora
# un lieve miglioramento delle performance a giustificazione ancora del fatto come
# il regressore TIT è fortemente legato all'uscita e come questo è legato in maniera non lineare
# alla stessa.
lasso_poly.pred = predict(lasso.mod, s = lambda1SE_lasso, newx = x[test_set, ])
ridge_poly.pred = predict(ridge.mod, s = lambda1SE_ridge, newx = x[test_set, ])
lasso_poly.pred_mse = mean((lasso_poly.pred-y[test_set])^2) #  2.06969
ridge_poly.pred_mse = mean((ridge_poly.pred-y[test_set])^2) #  2.126376
lasso_poly.pred_mse
ridge_poly.pred_mse
lasso.pred_mse
ridge.pred_mse
ridge.pred
lasso.mod
# Chapter 10 Lab 1
#### Principal Components Analysis (PCA) #####
# In this lab, we perform PCA on the USArrests data set, which is part of
# the base R package. The rows of the data set contain the 50 states, in
# alphabetical order.
states=row.names(USArrests); states
names(USArrests)
#### ---- PCA ---- ####
pr.out=prcomp(ds, scale=TRUE)
dim(pr.out$x)
pr.out$rotation
cor(ds)
#### ---- PCA ---- ####
pr.out=prcomp(ds~TIT+TEY, scale=TRUE)
#### ---- PCA ---- ####
pr.out=prcomp(ds, scale=TRUE)
cor(ds)
#### ---- PCA ---- ####
pr.out=prcomp(ds, scale=TRUE)
pr.out$rotation
names(USArrests)
# perform principal components analysis using the prcomp()
pr.out=prcomp(USArrests, scale=TRUE)
# By default, the prcomp() function centers the variables to have mean zero. By using the option scale=TRUE, we scale the variables to have standard deviation one.
names(pr.out)
pr.out$scale
# The center and scale components correspond to the means and standard deviations of the variables that were used for scaling prior to implementing PCA.
pr.out$rotation
pr.out$rotation # matrice di rotazione, ogni colonna rappresenta un loading vector
#### ---- PCA ---- ####
pr.out=prcomp(ds, scale=TRUE)
pr.out$rotation # matrice di rotazione, ogni colonna rappresenta un loading vector
biplot(pr.out, scale=0)
dev.new()
biplot(pr.out, scale=0)
pr.var=pr.out$sdev^2; pr.var # variance
#### ---- PCA ---- ####
ds_pca = subset(ds, select = -c(NOX))
#### ---- PCA ---- ####
ds_pca = subset(ds, select = -c(NOX,CO))
pr.out=prcomp(ds, scale=TRUE)
pr.out$rotation # matrice di rotazione, ogni colonna rappresenta un loading vector
#### ---- PCA ---- ####
ds_pca = subset(ds, select = -cCO))
#### ---- PCA ---- ####
ds_pca = subset(ds, select = -c(CO))
pr.out=prcomp(ds, scale=TRUE)
pr.out$rotation # matrice di rotazione, ogni colonna rappresenta un loading vector
#### ---- PCA ---- ####
ds_pca = subset(ds, select = -c(CO))
#### ---- PCA ---- ####
ds_pca = subset(ds, select = -c(NOX))
pr.out=prcomp(ds_pca, scale=TRUE)
pr.out$rotation # matrice di rotazione, ogni colonna rappresenta un loading vector
dev.new()
#### ---- PCA ---- ####
set.seed(1)
#### ---- PCA ---- ####
set.seed(1)
pcr.fit = pcr(CO~.-NOX, data = ds[training_set],scale = TRUE, validation = "CV")
library(pls)
pcr.fit = pcr(CO~.-NOX, data = ds[training_set],scale = TRUE, validation = "CV")
pcr.fit = pcr(CO~.-NOX, data = ds[training_set, ],scale = TRUE, validation = "CV")
dev.new()
validationplot(pcr.fit, val.type = "MSEP", legendpos = "topright")
pcr.fit = pcr(CO~.-NOX, data = ds,scale = TRUE, validation = "CV")
dev.new()
validationplot(pcr.fit, val.type = "MSEP", legendpos = "topright")
xx = c(1:nrow(ds))
train = (nrow(ds)*70)/100
test = (nrow(ds)*30)/100
# Sul training andiamo ad eseguire la CV per individuare il lambda miglire
# Sul test andremmo a valutare il modello ottenuto per fare un confronto Ridge vs Lasso
set.seed(1)
training_set_pca = sample(xx, train, rep = FALSE)
test_set_pca = sample(xx[-training_set], test, rep = FALSE)
# Sul training andiamo ad eseguire la CV per individuare il lambda miglire
# Sul test andremmo a valutare il modello ottenuto per fare un confronto Ridge vs Lasso
set.seed(1)
training_set = sample(xx, train, rep = FALSE)
test_set = sample(xx[-training_set], test, rep = FALSE)
set.seed(1)
pcr.fit = pcr(CO~.-NOX, data = ds,scale = TRUE, validation = "CV")
summary(pcr.fit)
set.seed(1)
pls.fit = plsr(CO~.-NOX, data = ds,scale = TRUE, validation = "CV")
summary(pls.fit)
summary(pls.fit)
training_set = sample(xx, train, rep = FALSE)
test_set = sample(xx[-training_set], test, rep = FALSE)
# Sul training andiamo ad eseguire la CV per individuare il lambda miglire
# Sul test andremmo a valutare il modello ottenuto per fare un confronto Ridge vs Lasso
set.seed(1)
training_set = sample(xx, train, rep = FALSE)
test_set = sample(xx[-training_set], test, rep = FALSE)
pcr.fit = pcr(CO~.-NOX, data = ds[training_set, ],scale = TRUE, validation = "CV")
summary(pcr.fit)
set.seed(1)
pcr.fit = pcr(CO~.-NOX, data = ds[training_set, ],scale = TRUE, validation = "CV")
summary(pcr.fit)
set.seed(1)
pls.fit = plsr(CO~.-NOX, data = ds[train_set, ],scale = TRUE, validation = "CV")
summary(pls.fit)
dev.new()
validationplot(pcr.fit, val.type = "MSEP", legendpos = "topright")
dev.new()
validationplot(pls.fit, val.type = "MSEP", legendpos = "topright")
minPCR = which.min(MSEP(pcr.fit)$val[1,,][-1])
minPCR # 5
which.min(MSEP(pls.fit)$val[1,,][-1])
dev.new()
plot(pcr.fit, ncomp = 5, asp = 1, line = TRUE)
dev.new()
plot(pls.fit, ncomp = 3, asp = 1, line = TRUE)
dev.new()
plot(pcr.fit, plottype = "scores", comps = 1:3)
explvar(pcr.fit)
dev.new()
plot(pls.fit, plottype = "scores", comps = 1:3)
explvar(pcr.fit)
#### ---- PLS + Poly ---- ####
set.seed(1)
pls.fit = plsr(CO~.-NOX-TIT+poly(TIT,2), data = ds[train_set, ],scale = TRUE, validation = "CV")
summary(pls.fit)
pls.fit = plsr(CO~.-NOX-TIT+poly(TIT,3), data = ds[train_set, ],scale = TRUE, validation = "CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit, val.type = "MSEP", legendpos = "topright")
#### ---- PLS + Poly ---- ####
set.seed(1)
pls.fit = plsr(CO~.-NOX-TIT+poly(TIT,2), data = ds[train_set, ],scale = TRUE, validation = "CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit, val.type = "MSEP", legendpos = "topright")
which.min(MSEP(pls.fit)$val[1,,][-1])
dev.new()
plot(pls.fit, ncomp = 3, asp = 1, line = TRUE)
set.seed(1)
pcr.fit = pcr(CO~.-NOX, data = ds[training_set, ],scale = TRUE, validation = "CV")
summary(pcr.fit)
dev.new()
validationplot(pcr.fit, val.type = "MSEP", legendpos = "topright")
which.min(MSEP(pcr.fit)$val[1,,][-1])
dev.new()
plot(pcr.fit, ncomp = 5, asp = 1, line = TRUE)
set.seed(1)
pls.fit = plsr(CO~.-NOX, data = ds[train_set, ],scale = TRUE, validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP", legendpos = "topright")
dev.new()
plot(pls.fit, plottype = "scores", comps = 1:3)
explvar(pcr.fit)
dev.new()
plot(pls.fit, ncomp = 3, asp = 1, line = TRUE)
dev.new()
plot(pcr.fit, ncomp = 5, asp = 1, line = TRUE)
set.seed(1)
pls.fit = plsr(CO~.-NOX-TIT+poly(TIT,2), data = ds[train_set, ],scale = TRUE, validation = "CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit, val.type = "MSEP", legendpos = "topright")
which.min(MSEP(pls.fit)$val[1,,][-1])
dev.new()
plot(pls.fit, ncomp = 2, asp = 1, line = TRUE)
