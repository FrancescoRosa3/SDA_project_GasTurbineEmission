#### ---- Poly 3 TIT only ---- ####
dev.new()
plot(TIT, CO)
y <- predict(fit_poly_3, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
ci_pred_TIT_poly = predict(fit_poly_3, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit_poly_3, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 1)
matplot(xx, ci_pred_TIT_poly$fit[,3],,lty=3,col="yellow",type="l",add = T, lwd = 1)
matplot(xx, pi_pred_TIT_poly[,2],,lty=3, col="violet", type = "l", add = T, lwd = 1)
matplot(xx, pi_pred_TIT_poly[,3],,lty=3, col="violet", type = "l", add = T, lwd = 1)
dev.new()
plot(TIT, CO)
y <- predict(fit_poly, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
ci_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 3)
matplot(xx, ci_pred_TIT_poly$fit[,3],,lty=3,col="yellow",type="l",add = T, lwd = 3)
matplot(xx, pi_pred_TIT_poly[,2],,lty=3, col="violet", type = "l", add = T, lwd = 3)
matplot(xx, pi_pred_TIT_poly[,3],,lty=3, col="violet", type = "l", add = T, lwd = 3)
dev.new()
plot(TIT, CO)
y <- predict(fit1.poly3, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
ci_pred_TIT_poly = predict(fit1.poly3, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit1.poly3, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 1)
matplot(xx, ci_pred_TIT_poly$fit[,3],,lty=3,col="yellow",type="l",add = T, lwd = 1)
matplot(xx, pi_pred_TIT_poly[,2],,lty=3, col="violet", type = "l", add = T, lwd = 1)
matplot(xx, pi_pred_TIT_poly[,3],,lty=3, col="violet", type = "l", add = T, lwd = 1)
#### ---- Error Distribution ---- ####
errors = predict(fit1, newdata = data.frame(ds[train_set, ]))
hist(errors)
#### ---- Error Distribution ---- ####
errors = (predict(fit1, newdata = data.frame(ds[train_set, ])) - ds[train_set, "CO"])^2)
#### ---- Error Distribution ---- ####
errors = (predict(fit1, newdata = data.frame(ds[train_set, ])) - ds[train_set, "CO"])^2
hist(errors)
plot(density(errors))
hist(errors)
plot(density(errors))
mean(erros)
mean(errors)
# Il modello senza i regressori di sopra ha un valore di R^2 paragonabile al modello totale,
# a dimostrazione del fatto che l'informazione portata da TEY,CDP e GTEP può essere spiegata da altri regressori
# e la F-statistic aumenta.
summary(fit1)
dev.new()
par(mfrow = c(2,2))
plot(fit1)
fit1.stderrors = rstandard(fit1)
hist(fit1.stderrors)
hist(fit1.stderrors, xlim = 10)
hist(fit1.stderrors)
plot(density(fit1.stderrors))
#### ---- How confidence and prediction intervals change ---- ####
# Andiamo a confrontare come gli intervalli di confidenza e predizione sulla risposta CO, rispetto al regressore
# TIT, per il quale è stato osservato un legame con CO, cambiano rispetto alle varie modifiche apportate al modello
# con l'aspettativa che:
# 1. Introducendo il termine quadratico, l'intervallo si restringa, rispetto alla semplice regressione lineare;
# 2. Intervalli non si modificano con l'introduzione degli altri regressori, in quanto è stato osservato come in realtà a dominare
# è il termine polinomiale
xx <- seq(min(TIT), max(TIT), along.with = TIT)
ci_pred_TIT = predict(fit_tit, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT = predict(fit_tit, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
dev.new()
plot(TIT, CO)
#### ---- Linear only TIT ---- ####
abline(fit_tit, col = "blue")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T, lwd = 3)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T, lwd = 3)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T, lwd = 3)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T, lwd = 3)
#### ---- Poly 2 TIT only ---- ####
# Si osserva come effettivamente con l'introduzione del polinomio
# gli intervalli di predizione seguono di più l'andamento della curva dei dati
# andando a restringere l'intervallo di predizione
dev.new()
plot(TIT, CO)
y <- predict(fit_poly, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
ci_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit_poly, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 3)
matplot(xx, ci_pred_TIT_poly$fit[,3],,lty=3,col="yellow",type="l",add = T, lwd = 3)
matplot(xx, pi_pred_TIT_poly[,2],,lty=3, col="violet", type = "l", add = T, lwd = 3)
matplot(xx, pi_pred_TIT_poly[,3],,lty=3, col="violet", type = "l", add = T, lwd = 3)
#### ---- Poly 2 TIT with others ---- ####
# A dimostrazione del fatto che gli altri predittori risultano ininfluenti, osserviamo come
# l'andamento del polinomio di grado 2 con la presenza degli altri predittori è praticamente irrilevante
dev.new()
plot(TIT, CO)
y <- predict(fit1.poly2, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
ci_pred_TIT_poly = predict(fit1.poly2, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit1.poly2, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 1)
matplot(xx, ci_pred_TIT_poly$fit[,3],,lty=3,col="yellow",type="l",add = T, lwd = 1)
matplot(xx, pi_pred_TIT_poly[,2],,lty=3, col="violet", type = "l", add = T, lwd = 1)
matplot(xx, pi_pred_TIT_poly[,3],,lty=3, col="violet", type = "l", add = T, lwd = 1)
dev.new()
plot(TIT, CO)
y <- predict(fit_poly_3, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
ci_pred_TIT_poly = predict(fit_poly_3, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit_poly_3, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 1)
matplot(xx, ci_pred_TIT_poly$fit[,3],,lty=3,col="yellow",type="l",add = T, lwd = 1)
matplot(xx, pi_pred_TIT_poly[,2],,lty=3, col="violet", type = "l", add = T, lwd = 1)
matplot(xx, pi_pred_TIT_poly[,3],,lty=3, col="violet", type = "l", add = T, lwd = 1)
dev.new()
plot(TIT, CO)
y <- predict(fit1.poly3, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
ci_pred_TIT_poly = predict(fit1.poly3, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit1.poly3, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT_poly$fit[,2],,lty=3,col="yellow",type="l",add = T, lwd = 1)
matplot(xx, ci_pred_TIT_poly$fit[,3],,lty=3,col="yellow",type="l",add = T, lwd = 1)
matplot(xx, pi_pred_TIT_poly[,2],,lty=3, col="violet", type = "l", add = T, lwd = 1)
matplot(xx, pi_pred_TIT_poly[,3],,lty=3, col="violet", type = "l", add = T, lwd = 1)
# Il modello senza i regressori di sopra ha un valore di R^2 paragonabile al modello totale,
# a dimostrazione del fatto che l'informazione portata da TEY,CDP e GTEP può essere spiegata da altri regressori
# e la F-statistic aumenta.
summary(fit1)
summary(fit_tit)
#### ---- Linear TIT with others ---- ####
dev.new()
plot(TIT, CO)
abline(fit1, col = "blue")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T, lwd = 3)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T, lwd = 3)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T, lwd = 3)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T, lwd = 3)
#### ---- Linear TIT with others ---- ####
dev.new()
plot(TIT, CO)
y <- predict(fit1, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
ci_pred_TIT_poly = predict(fit1, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT_poly = predict(fit1, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T, lwd = 3)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T, lwd = 3)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T, lwd = 3)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T, lwd = 3)
dev.new()
plot(TIT, CO)
y <- predict(fit1, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
ci_pred_TIT = predict(fit1, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT = predict(fit1, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T, lwd = 3)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T, lwd = 3)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T, lwd = 3)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T, lwd = 3)
dev.new()
plot(TIT, CO)
y <- predict(fit1, newdata = data.frame(TIT = x))
lines(x, y, col = "red")
ci_pred_TIT = predict(fit1, newdata = data.frame(TIT = xx), se.fit = T, interval = "confidence")
pi_pred_TIT = predict(fit1, newdata = data.frame(TIT = xx), set.fit = T, interval = "prediction")
matplot(xx, ci_pred_TIT$fit[,2],,lty=3,col="red",type="l",add = T, lwd = 1)
matplot(xx, ci_pred_TIT$fit[,3],,lty=3,col="red",type="l",add = T, lwd = 1)
matplot(xx, pi_pred_TIT[,2],,lty=3, col="green", type = "l", add = T, lwd = 1)
matplot(xx, pi_pred_TIT[,3],,lty=3, col="green", type = "l", add = T, lwd = 1)
val.err = matrix(data=NA,nrow=10,ncol=10)
?regsubset
?regsubsets
libray(regsubset)
libray(leaps)
library(leaps)
?regsubsets
ncol(ds)
n = nrow(ds)
training = (n*70)/100
n = nrow(ds)
training = (n*70)/100
lm_fit = lm(CO~TIT, data = ds, subset = training)
val.err[i, j] = mean(predict(lm_fit, newdata = ds[-train, ]) - ds[-train, "CO"])^2)
val.err[i, j] = mean((predict(lm_fit, newdata = ds[-train, ]) - ds[-train, "CO"])^2)
for(i in 1:5){
for(j in 1:5)  {
set.seed(i)
train = sample(1:n, training)
lm_fit = lm(CO~poly(TIT, j), data = ds, subset = training)
val.err[i, j] = mean((predict(lm_fit, newdata = ds[-train, ]) - ds[-train, "CO"])^2)
}
}
set.seed(i)
train = sample(1:n, training)
lm_fit = lm(CO~poly(TIT, j), data = ds, subset = training)
val.err[i, j] = mean((predict(lm_fit, newdata = ds[-train, ]) - ds[-train, "CO"])^2)
val.err = matrix(data=NA,nrow=5,ncol=5)
n = nrow(ds)
training = (n*70)/100
for(i in 1:5){
for(j in 1:5)  {
set.seed(i)
train = sample(1:n, training)
lm_fit = lm(CO~poly(TIT, j), data = ds, subset = training)
val.err[i, j] = mean((predict(lm_fit, newdata = ds[-train, ]) - ds[-train, "CO"])^2)
}
}
val.err[i, j] = mean((predict(lm_fit, newdata = ds[-train, ]) - ds[-train, "CO"])^2)
print(j)
summary(lm_fit)
summary(lm(CO~poly(TIT, 0), data = ds, subset = training))
summary(lm(CO~poly(TIT, 1), data = ds, subset = training))
lm_fit = lm(CO~poly(TIT, j), data = ds, subset = train)
summary(lm(CO~poly(TIT, 1), data = ds, subset = training))
summary(lm(CO~poly(TIT, 1), data = ds, subset = train))
for(i in 1:5){
for(j in 1:5)  {
set.seed(i)
train = sample(1:n, training)
print(j)
lm_fit = lm(CO~poly(TIT, j), data = ds, subset = train)
val.err[i, j] = mean((predict(lm_fit, newdata = ds[-train, ]) - ds[-train, "CO"])^2)
}
}
for(i in 1:5){
for(j in 1:5)  {
set.seed(i)
train = sample(1:n, training)
lm_fit = lm(CO~poly(TIT, j), data = ds, subset = train)
val.err[i, j] = mean((predict(lm_fit, newdata = ds[-train, ]) - ds[-train, "CO"])^2)
}
}
val.err
plot(seq(1:5), val.err)
plot(seq(1:5), seq(min(val.err):max(val.err)))
seq(min(val.err):max(val.err)
plot(c(1:5), seq(min(val.err):max(val.err)))
c(1:5)
c(min(val.err):max(val.err)
c(min(val.err):max(val.err)
c(min(val.err):max(val.err)
plot(c(1:5),
))
min(val.err)
max(val.err)
plot(c(1:5), seq(min(val.err), max(val.err), length = 10)
c
plot(c(1:5), seq(min(val.err), max(val.err), length = 10))
val.err
val.err = matrix(data=NA,nrow=10,ncol=5)
n = nrow(ds)
training = (n*70)/100
for(i in 1:10){
for(j in 1:5)  {
set.seed(i)
train = sample(1:n, training)
lm_fit = lm(CO~poly(TIT, j), data = ds, subset = train)
val.err[i, j] = mean((predict(lm_fit, newdata = ds[-train, ]) - ds[-train, "CO"])^2)
}
}
val.err
dev.new()
lines(1:5, val.err[i,j])
lines(range(1:5), val.err[i,j])
range(1:5)
lines(c(1:5), val.err[i,j])
c(1:5)
val.err[i,j]
lines(c(1:5), val.err[i,:])
lines(c(1:5), val.err[i,])
plot(c(1:5), val.err[i,])
dev.new()
for(i in 1:5){
for(j in 1:5){
plot(c(1:5), val.err[i,])
}
}
plot(c(1:5), val.err[i,],type = "p")
plot(c(1:5), val.err[i,],type = "l")
lines(c(1:5), val.err[i,])
lines(c(1:5), val.err[i,])
for(i in 2:5){
for(j in 2:5){
lines(c(1:5), val.err[i,])
}
}
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE")
for(i in 2:5){
for(j in 2:5){
lines(c(1:5), val.err[i,])
}
}
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red")
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = 0)
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = c(min(val.err):max(val.er))
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = c(min(val.err):max(val.er)))
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.er)))
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.er)))
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.err)))
for(i in 2:10){
for(j in 2:5){
lines(c(1:5), val.err[i,])
}
}
for(i in 2:5){
for(j in 2:5){
lines(c(1:5), val.err[i,])
}
}
dev.new()
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.err)))
for(i in 2:5){
for(j in 2:5){
lines(c(1:5), val.err[i,])
}
}
col = ["violet" "black" "orance" "yellow" "green" "black" "pink"]
col = c("violet", "black", "orance", "yellow", "green", "black", "pink", "blue", "pink")
dev.new()
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.err)))
col = c("violet", "black", "orance", "yellow", "green", "black", "pink", "blue", "pink")
for(i in 2:10){
for(j in 2:5){
lines(c(1:5), val.err[i,], col = col[i])
}
}
dev.new()
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.err)))
col = c("violet", "black", "orange", "yellow", "green", "black", "pink", "blue", "pink")
for(i in 2:10){
for(j in 2:5){
lines(c(1:5), val.err[i,], col = col[i])
}
}
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.err)), lw = 3)
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.err)), lwd = 3)
dev.new()
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.err)), lwd = 3)
col = c("violet", "black", "orange", "yellow", "green", "black", "pink", "blue", "pink")
for(i in 2:10){
for(j in 2:5){
lines(c(1:5), val.err[i,], col = col[i], lwd = 3)
}
}
train.err = matrix(data=NA, nrow = 10, ncol = 5)
dev.new()
plot(c(1:5), train.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.err)), lwd = 3)
col = c("violet", "black", "orange", "yellow", "green", "black", "pink", "blue", "pink")
for(i in 2:10){
for(j in 2:5){
lines(c(1:5), train.err[i,], col = col[i], lwd = 3)
}
}
for(i in 1:10){
for(j in 1:5)  {
set.seed(i)
train = sample(1:n, training)
lm_fit = lm(CO~poly(TIT, j), data = ds, subset = train)
train.err[i, j] = mean((predict(lm_fit, newdata = ds[train, ]) - ds[train, "CO"])^2)
val.err[i, j] = mean((predict(lm_fit, newdata = ds[-train, ]) - ds[-train, "CO"])^2)
}
}
dev.new()
plot(c(1:5), train.err[1,],type = "l", xlab = "polynomial degree", ylab = "MSE", col = "red", ylim = range(min(val.err):max(val.err)), lwd = 3)
col = c("violet", "black", "orange", "yellow", "green", "black", "pink", "blue", "pink")
for(i in 2:10){
for(j in 2:5){
lines(c(1:5), train.err[i,], col = col[i], lwd = 3)
}
}
dev.new()
plot(c(1:5), train.err[1,],type = "l", xlab = "polynomial degree", ylab = "train MSE", col = "red", ylim = range(min(val.err):max(val.err)), lwd = 3)
col = c("violet", "black", "orange", "yellow", "green", "black", "pink", "blue", "pink")
for(i in 2:10){
for(j in 2:5){
lines(c(1:5), train.err[i,], col = col[i], lwd = 3)
}
}
#### ---- Variabilità Leave One Out Validation Set Approach ---- ####
library(glm)
#### ---- Variabilità Leave One Out Validation Set Approach ---- ####
library(boot)
lm_fit = glm(CO~poly(TIT, j), data = ds, subset = train)
#### ---- Variabilità Leave One Out Validation Set Approach ---- ####
looval.err = matrix(data=NA,nrow=10,ncol=5)
library(boot)
for(i in 1:10){
for(j in 1:5)  {
glm_fit = glm(CO~poly(TIT, j), data = ds)
looVal.err[i, j] = cv.glm(ds, glm_fit)$delta[1]
}
}
for(j in 1:5)  {
glm_fit = glm(CO~poly(TIT, j), data = ds)
looVal.err[i, j] = cv.glm(ds, glm_fit)$delta[1]
}
#### ---- K-Fold Cross Validation ---- ####
set.seed(1)
val.err = matrix(data=NA,nrow=10,ncol=5)
n = nrow(ds)
for(i in 1:10){
for(j in 1:5)  {
set.seed(i)
glm_fit = glm(CO~poly(TIT, j), data = ds, subset = train)
val.err[i, j] = cv.glm(ds, glmfit, k = 10)
}
}
val.err[i, j] = cv.glm(ds, glmfit, K = 10)
val.err[i, j] = cv.glm(ds, glm_fit, K = 10)
for(i in 1:10){
for(j in 1:5)  {
set.seed(i)
glm_fit = glm(CO~poly(TIT, j), data = ds, subset = train)
val.err[i, j] = cv.glm(ds, glm_fit, K = 10)
}
}
for(i in 1:10){
for(j in 1:5)  {
set.seed(i)
glm_fit = glm(CO~poly(TIT, j), data = ds)
val.err[i, j] = cv.glm(ds, glm_fit, K = 10)
}
}
val.err[i, j] = cv.glm(ds, glm_fit, K = 10)$delta[1]
for(i in 1:10){
for(j in 1:5)  {
set.seed(i)
glm_fit = glm(CO~poly(TIT, j), data = ds)
val.err[i, j] = cv.glm(ds, glm_fit, K = 10)$delta[1]
}
}
for(i in 1:10){
for(j in 1:5)  {
print(i,j)
set.seed(i)
glm_fit = glm(CO~poly(TIT, j), data = ds)
val.err[i, j] = cv.glm(ds, glm_fit, K = 10)$delta[1]
}
}
dev.new()
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "train MSE", col = "red", ylim = range(min(val.err):max(val.err)), lwd = 3)
col = c("violet", "black", "orange", "yellow", "green", "black", "pink", "blue", "pink")
for(i in 2:10){
for(j in 2:5){
lines(c(1:5), val.err[i,], col = col[i], lwd = 3)
}
}
min(val.err)
ange(min(val.err):max(val.err))
range(min(val.err):max(val.err))
max(val.err)
dev.new()
plot(c(1:5), val.err[1,],type = "l", xlab = "polynomial degree", ylab = "train MSE", col = "red", ylim = range(min(val.err):max(val.err)), lwd = 3)
set.seed(1)
kval.err = matrix(data=NA,nrow=10,ncol=5)
n = nrow(ds)
for(i in 1:10){
for(j in 1:5)  {
print(i)
set.seed(i)
glm_fit = glm(CO~poly(TIT, j), data = ds)
kval.err[i, j] = cv.glm(ds, glm_fit, K = 10)$delta[1]
}
}
dev.new()
plot(c(1:5), kval.err[1,],type = "l", xlab = "polynomial degree", ylab = "train MSE", col = "red", ylim = range(min(kval.err):max(kval.err)), lwd = 3)
col = c("violet", "black", "orange", "yellow", "green", "black", "pink", "blue", "pink")
for(i in 2:10){
for(j in 2:5){
lines(c(1:5), kval.err[i,], col = col[i], lwd = 3)
}
}
range(min(kval.err):max(kval.err))
plot(c(1:5), kval.err[1,],type = "l", xlab = "polynomial degree", ylab = "train MSE", col = "red", ylim = range(min(kval.err),max(kval.err)), lwd = 3)
dev.new()
plot(c(1:5), kval.err[1,],type = "l", xlab = "polynomial degree", ylab = "train MSE", col = "red", ylim = range(min(kval.err),max(kval.err)), lwd = 3)
col = c("violet", "black", "orange", "yellow", "green", "black", "pink", "blue", "pink")
for(i in 2:10){
for(j in 2:5){
lines(c(1:5), kval.err[i,], col = col[i], lwd = 3)
}
}
#### ---- K-fold CV sui modelli generati ---- ####
fit_all_cv = cv.glm(ds, fit_all_cv, K = 10)$delta[1]
#### ---- K-fold CV sui modelli generati ---- ####
fit_all_cv = cv.glm(ds, fit_all, K = 10)$delta[1]
fit_all_cv
cv.glm(ds, fit_all, K = 10)
#### ---- K-fold CV sui modelli generati ---- ####
# fit_all, fit1, fit_poly, fit1.poly2, fit1.poly3
summary(fit_all)
glm_fit_all = glm(formula = CO ~ . - NOX, ds)
glm_fit_all = glm(ds$CO~.-ds$NOX, ds)
glm_fit_all = glm(CO~.-$NOX, ds)
glm_fit_all = glm(CO~.-NOX, ds)
glm_fit_all = glm(CO~NOX, ds)
glm_fit_all = glm(CO~.NOX, data.ds)
fit_all_cv = cv.glm(ds, fit_all, K = 10)$delta[1]
fit_all_cv
glm_fit_all = glm(formula = CO~.NOX, data.ds)
glm_fit_all = glm(formula = CO~.NOX, data = ds)
glm_fit_all = glm(formula = CO~.-NOX, data = ds)
fit_all_cv = cv.glm(ds, fit_all, K = 10)$delta[1]
fit_all_cv
fit_all_cv
fit_all_cv = cv.glm(ds, glm_fit_all, K = 10)$delta[1]
fit_all_cv
glm_fit1 = glm(formula = CO~.-NOX-TEY-CDP-GTEP, data = ds)
fit_1_cv = cv.glm(ds, glm_fit1, K = 10)$delta[1]
fit_1_cv
glm_fit_poly = glm(formula = CO~poly(TIT,2), data = ds)
fit_poly_cv = cv.glm(ds, glm_fit_poly, K = 10)$delta[1] # 2.27
fit_poly_cv
glm_fit1_poly2 = glm(formula = CO~.-NOX-TEY-CDP-GTEP-TIT+poly(TIT, 2), data = ds)
fit1_poly2_cv = cv.glm(ds, glm_fit1_poly2, K = 10)$delta[1] # 2.021183
fit1_poly2_cv
